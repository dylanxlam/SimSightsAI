{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  # for nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Type Conversion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Types\n",
    "data_types = data.dtypes\n",
    "print(\"Data Types for Each Column in Your Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def convert_data_types(data):\n",
    "  \"\"\"\n",
    "  Prints data types for each column and allows user to change them.\n",
    "\n",
    "  Args:\n",
    "      data (pandas.DataFrame): The DataFrame containing the data.\n",
    "\n",
    "  Returns:\n",
    "      pandas.DataFrame: The DataFrame potentially with changed data types.\n",
    "  \"\"\"\n",
    "\n",
    "  # Explain data types in a dictionary for easy reference\n",
    "  dtype_explanations = {\n",
    "      'int64': \"Integer (whole numbers, positive or negative)\",\n",
    "      'float64': \"Decimal number\",\n",
    "      'object': \"Text data (strings)\",\n",
    "      'category': \"Categorical data (limited set of options)\",\n",
    "      'datetime64[ns]': \"Date and time\",\n",
    "      'bool': \"Boolean (True or False)\"\n",
    "  }\n",
    "\n",
    "  # Print data types with explanations\n",
    "  for col, dtype in data_types.items():\n",
    "    print(f\"- {col}: {dtype} ({dtype_explanations.get(dtype, 'Unknown')})\")\n",
    "\n",
    "  # Prompt user for data type changes\n",
    "  change_dtypes = input(\"Would you like to change any data types (y/n)? \").lower()\n",
    "  if change_dtypes == \"y\":\n",
    "    while True:\n",
    "      # Ask for column and desired data type\n",
    "      col_to_change = input(\"Enter the column name to change the data type: \").lower()\n",
    "      new_dtype = input(\"Enter the desired new data type (int, float, object, etc.): \").lower()\n",
    "\n",
    "      # Check if column exists and new data type is valid\n",
    "      if col_to_change in data.columns and new_dtype in dtype_explanations.keys():\n",
    "        try:\n",
    "          # Attempt conversion (handles potential errors)\n",
    "          data[col_to_change] = data[col_to_change].astype(new_dtype)\n",
    "          print(f\"Data type for '{col_to_change}' changed to {new_dtype}.\")\n",
    "          # **Modified break logic:**\n",
    "          break_loop = input(\"Do you want to convert another column (y/n)? \").lower()\n",
    "          if break_loop != \"y\":\n",
    "            break\n",
    "        except (ValueError, TypeError) as e:\n",
    "          print(f\"Error converting '{col_to_change}' to {new_dtype}: {e}\")\n",
    "          # **Prompt to continue after error**\n",
    "          continue_loop = input(\"Would you like to try converting another column (y/n)? \").lower()\n",
    "          if continue_loop != \"y\":\n",
    "            break\n",
    "      else:\n",
    "        print(f\"Invalid column name or data type. Please try again.\")\n",
    "\n",
    "  return data\n",
    "\n",
    "# Example usage \n",
    "data = convert_data_types(data.copy())  # Avoid modifying original data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focus: Data cleaning addresses inconsistencies, errors, and missing values within the data itself.\n",
    "\n",
    "Data Type Conversion: In this context, converting data types is often a cleaning step when the data type is incorrect or incompatible with how the data should be represented.\n",
    "Examples:\n",
    "- Inconsistent date formats (text to datetime).\n",
    "- Text values in numerical columns (text to numerical).\n",
    "- Incorrect data types due to import issues (e.g., strings instead of integers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dealing With Normality and Skewness**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most efficient way to assess normality and skewness in your data columns depends on a few factors:\n",
    "\n",
    "**1. Number of Columns:**\n",
    "\n",
    "* **Few Columns:** `(still need implementation)` If you have a small number of columns (less than 10), visual inspection using histograms and QQ plots might be the most efficient approach. These techniques are easy to understand and interpret, providing a quick grasp of the data distribution.\n",
    "\n",
    "* **Many Columns:** With a large number of columns (more than 10), visual inspection becomes cumbersome. Here, statistical tests like the Shapiro-Wilk test can be more efficient. You can calculate the test statistic and p-value for each column to identify potential deviations from normality. A threshold for the p-value (e.g., 0.05) can be used to decide if the data is likely non-normal.\n",
    "\n",
    "**2. Desired Level of Detail:**\n",
    "\n",
    "* **Basic Assessment:** If you just need a quick indication of normality or skewness, histograms and statistical tests with p-values provide a sufficient level of detail.\n",
    "\n",
    "* **Detailed Analysis:** For a more in-depth analysis, you can combine both approaches. Start with histograms and QQ plots to get a visual sense of the distribution, and then follow up with statistical tests to confirm your observations or explore borderline cases with p-values close to the chosen threshold.\n",
    "\n",
    "Here's a breakdown of the efficiency considerations:\n",
    "\n",
    "| Method | Efficiency for Few Columns | Efficiency for Many Columns | Level of Detail |\n",
    "|---|---|---|---|\n",
    "| Histograms & QQ Plots | High (easy to interpret visually) | Low (time-consuming for many columns) | High (visual assessment of shape) |\n",
    "| Statistical Tests | Low (requires calculations) | High (efficient for many columns) | Moderate (p-value indicates normality likelihood) |\n",
    "\n",
    "**Combined Approach:**\n",
    "\n",
    "In practice, a combination of visual inspection and statistical tests often offers the best balance between efficiency and detail. Start with histograms and QQ plots for a quick overview, then use statistical tests for more rigorous confirmation, especially when dealing with many columns.\n",
    "\n",
    "Here are some additional factors to consider:\n",
    "\n",
    "* **Computational Resources:** If computational resources are limited, visual methods might be preferred. Statistical tests, especially for large datasets, can require more processing power.\n",
    "* **Domain Knowledge:** If you have domain knowledge about the data, you might have an initial expectation about the normality of certain features. This can guide your choice of method, focusing on tests for features where normality is critical for your analysis.\n",
    "\n",
    "Ultimately, the most efficient approach depends on your specific needs and the size of your dataset. Combining visual and statistical methods often provides a comprehensive and efficient way to assess normality and skewness in your data columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Normalizing/Scaling Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "def scale(data):\n",
    "  \"\"\"\n",
    "  Identifies skewed features, suggests corrections, and performs scaling/normalization.\n",
    "\n",
    "  Args:\n",
    "      data (pandas.DataFrame): The DataFrame containing the data.\n",
    "\n",
    "  Returns:\n",
    "      pandas.DataFrame: The transformed DataFrame with addressed skewness and scaling/normalization.\n",
    "  \"\"\"\n",
    "  numerical_cols = data.select_dtypes(include=[np.number])\n",
    "  skewed_cols = []  # List to store column names with skewness\n",
    "\n",
    "  # Threshold for skewness (adjust as needed)\n",
    "  skewness_threshold = 0.5\n",
    "\n",
    "  for col in numerical_cols:\n",
    "    # Calculate skewness\n",
    "    skew = data[col].skew()\n",
    "    if abs(skew) > skewness_threshold:\n",
    "      skewed_cols.append(col)\n",
    "      print(f\"Column '{col}' appears skewed (skewness: {skew:.2f}).\")\n",
    "\n",
    "\n",
    "      # Inform decision-making\n",
    "      print(\"Here's a brief explanation of the available correction methods:\")\n",
    "      print(\"  - Log transformation (log(x + 1)): This method is often effective for right-skewed data (where values are concentrated on the left side of the distribution).\")\n",
    "      print(\"    It compresses the larger values and stretches the smaller ones, aiming for a more symmetrical distribution.\")\n",
    "      print(\"  - Square root transformation (sqrt(x)): This method can be helpful for moderately skewed data, positive-valued features, or data with a large number of zeros.\")\n",
    "      print(\"    It reduces the influence of extreme values and can bring the distribution closer to normality.\")\n",
    "      print(\"**Please consider the characteristics of your skewed feature(s) when making your choice.**\")\n",
    "      print(\"If you're unsure, you can experiment with both methods and compare the results visually (e.g., using histograms) to see which one normalizes the data more effectively for your specific case.\")\n",
    "\n",
    "      # User prompt for addressing skewness\n",
    "      action = input(\"Do you want to address the skewness (y/n)? \").lower()\n",
    "      if action == \"y\":\n",
    "        \n",
    "\n",
    "        # User chooses to address skewness\n",
    "        while True:  # Loop until a valid choice is made\n",
    "          fix_method = input(\"Choose a correction method (log/sqrt/none): \").lower()\n",
    "          if fix_method in [\"log\", \"sqrt\"]:\n",
    "            # Apply transformation (log or sqrt)\n",
    "            if fix_method == \"log\":\n",
    "              data[col] = np.log(data[col] + 1)  # Avoid log(0) errors by adding 1\n",
    "              print(f\"Applied log transformation to column '{col}'.\")\n",
    "            else:\n",
    "              data[col] = np.sqrt(data[col])\n",
    "              print(f\"Applied square root transformation to column '{col}'.\")\n",
    "            break  # Exit the loop if a valid choice is made\n",
    "          else:\n",
    "            print(\"Invalid choice. Please choose 'log', 'sqrt', or 'none'.\")\n",
    "\n",
    "      else:\n",
    "        print(f\"Skewness in '{col}' remains unaddressed.\")\n",
    "    \n",
    "    if not skewed_cols:\n",
    "      print(\"No significant skewness detected in numerical columns.\")\n",
    "\n",
    "  # User prompt for scaling/normalization (if applicable)\n",
    "  if len(numerical_cols) > 0:\n",
    "\n",
    "    print(\"Here's a brief explanation of the available scaling/normalization methods:\")\n",
    "    print(\"  - Standard scaling: This method transforms features by subtracting the mean and dividing by the standard deviation.\")\n",
    "    print(\"    This results in features centered around zero with a standard deviation of 1.\")\n",
    "    print(\"    It's suitable for algorithms that assume a normal distribution of features (e.g., Logistic Regression, Support Vector Machines).\")\n",
    "    print(\"  - Min-max scaling: This method scales each feature to a specific range, typically between 0 and 1.\")\n",
    "    print(\"    It achieves this by subtracting the minimum value and then dividing by the difference between the maximum and minimum values in the feature.\")\n",
    "    print(\"    This can be useful for algorithms that are sensitive to the scale of features (e.g., K-Nearest Neighbors).\")\n",
    "    print(\"**Choosing the right method depends on your data and the algorithm you're using.**\")\n",
    "    print(\"  - If you're unsure about the underlying distribution of your data, standard scaling might be a safer choice as it doesn't make assumptions about normality.\")\n",
    "    print(\"  - If your algorithm is sensitive to feature scales and doesn't assume normality, min-max scaling might be preferable.\")\n",
    "    print(\"Consider the characteristics of your data and algorithm when making your decision. You can also experiment with both methods\")\n",
    "    print(\"and compare the results using model performance metrics to see which one works best for your specific case.\")\n",
    "\n",
    "    action = input(\"Do you want to scale or normalize the numerical features (y/n)? \").lower()\n",
    "    if action == \"y\":\n",
    "\n",
    "      while True:  # Loop until a valid choice is made\n",
    "        method = input(\"Choose scaling/normalization method (standard/minmax/skip): \").lower()\n",
    "        if method in [\"standard\", \"minmax\"]:\n",
    "          if method == \"standard\":\n",
    "            scaler = StandardScaler()\n",
    "            data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "            print(f\"Applied standard scaling to numerical features.\")\n",
    "          else:\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "            print(f\"Applied min-max scaling to numerical features (range 0-1).\")\n",
    "          break  # Exit the loop if a valid choice is made\n",
    "        elif method == \"skip\":\n",
    "          print(\"Skipping scaling/normalization.\")\n",
    "          break\n",
    "        else:\n",
    "          print(\"Invalid choice. Please choose 'standard', 'minmax', or 'skip'.\")\n",
    "\n",
    "\n",
    "  if not skewed_cols:\n",
    "    print(\"No significant skewness detected in numerical columns.\")\n",
    "  return data\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "preprocessed_data = scale(data.copy())  # Operate on a copy to avoid modifying original data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scale` function offers a user-guided approach to data preprocessing, addressing both skewness in numerical features and optional scaling/normalization. Here's a breakdown of its functionalities:\n",
    "\n",
    "1. **Skewness Identification:**\n",
    "   - It identifies features with significant skewness (asymmetry in the distribution) based on a user-defined threshold.\n",
    "   - For skewed features, it informs the user about the skewness value and prompts them to address it.\n",
    "\n",
    "2. **Skewness Correction (Optional):**\n",
    "   - If the user chooses to address skewness, it offers options for log or square root transformation to potentially reduce skewness.\n",
    "   - The chosen transformation is applied to the specific feature(s).\n",
    "\n",
    "3. **Scaling/Normalization (Optional):**\n",
    "   - After addressing skewness (or if no skewness is found), it prompts the user to decide if they want to scale or normalize the numerical features.\n",
    "   - If the user chooses to proceed, it offers options for standard scaling (centering and scaling features to have zero mean and unit variance) or min-max scaling (scaling features to a specific range, typically 0-1).\n",
    "   - The chosen scaling/normalization method is applied using scikit-learn's `StandardScaler` or `MinMaxScaler` to transform the numerical features.\n",
    "\n",
    "\n",
    "Overall, this function simplifies data preprocessing by guiding the user through common steps like handling skewness and applying scaling/normalization techniques. It provides flexibility and explanations, making it easier to understand and customize the preprocessing process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Creating Interaction Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interaction_features(data, categorical_cols=None):\n",
    "  \"\"\"\n",
    "  Creates interaction features from categorical columns in a DataFrame.\n",
    "\n",
    "  Args:\n",
    "      data (pandas.DataFrame): The DataFrame containing the data.\n",
    "      categorical_cols (list, optional): A list of column names to consider for interaction features. If None, all categorical columns will be used. Defaults to None.\n",
    "\n",
    "  Returns:\n",
    "      pandas.DataFrame: The DataFrame with additional interaction features.\n",
    "  \"\"\"\n",
    "\n",
    "  if categorical_cols is None:\n",
    "    categorical_cols = [col for col in data.columns if data[col].dtype == 'category']\n",
    "\n",
    "  if not categorical_cols:\n",
    "    print(\"No categorical columns found in the data. Skipping interaction feature creation.\")\n",
    "    return data\n",
    "\n",
    "  # Display recommendations before prompting user\n",
    "  print(\"** Recommendations for Interaction Features:**\")\n",
    "  print(\"- Interaction features can capture complex relationships, potentially improving model performance.\")\n",
    "  print(\"- However, creating all possible interactions can lead to data sparsity and longer training times.\")\n",
    "  print(\"- Consider your domain knowledge to prioritize specific interactions.\")\n",
    "  print(\"- Start with a smaller set and use feature selection techniques for better interpretability.\")\n",
    "\n",
    "\n",
    "  # Get user confirmation to proceed\n",
    "  action = input(\"Do you want to create interaction features from categorical columns (y/n)? \").lower()\n",
    "  if action != \"y\":\n",
    "    print(\"Skipping interaction feature creation.\")\n",
    "    return data\n",
    "\n",
    "  # Prompt user to choose specific columns or create all possible interactions\n",
    "  while True:\n",
    "    choice = input(\"Choose interaction feature creation method (all/specific): \").lower()\n",
    "    if choice in [\"all\", \"specific\"]:\n",
    "      break\n",
    "    else:\n",
    "      print(\"Invalid choice. Please choose 'all' or 'specific'.\")\n",
    "\n",
    "  if choice == \"all\":\n",
    "    # Create all pairwise interaction features\n",
    "    for col1 in categorical_cols:\n",
    "      for col2 in categorical_cols:\n",
    "        if col1 != col2:\n",
    "          data[f\"{col1}_x_{col2}\"] = data[col1].astype(str) + \"_\" + data[col2].astype(str)\n",
    "    print(\"Created all possible pairwise interaction features.\")\n",
    "\n",
    "  else:\n",
    "    # Prompt user to choose specific columns for interaction\n",
    "    selected_cols = []\n",
    "    while True:\n",
    "      col_name = input(\"Enter a categorical column name (or 'done' to finish): \").lower()\n",
    "      if col_name == \"done\":\n",
    "        if not selected_cols:\n",
    "          print(\"No columns selected. Skipping interaction feature creation.\")\n",
    "        else:\n",
    "          for col1 in selected_cols:\n",
    "            for col2 in selected_cols:\n",
    "              if col1 != col2:\n",
    "                data[f\"{col1}_x_{col2}\"] = data[col1].astype(str) + \"_\" + data[col2].astype(str)\n",
    "          print(f\"Created interaction features for selected columns: {', '.join(selected_cols)}\")\n",
    "        break\n",
    "      elif col_name in categorical_cols:\n",
    "        selected_cols.append(col_name)\n",
    "        print(f\"Column '{col_name}' added for interaction features.\")\n",
    "      else:\n",
    "        print(f\"Invalid column name: '{col_name}'. Please choose from categorical columns.\")\n",
    "\n",
    "  return data\n",
    "\n",
    "create_interaction_features(data.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `create_interaction_features` function offers a user-guided approach to creating interaction features from categorical columns in a DataFrame. Here's a breakdown of its functionalities:\n",
    "\n",
    "- **Input:** It takes a DataFrame (`data`) and an optional list of categorical column names (`categorical_cols`).\n",
    "- **Categorical Column Identification:** If no `categorical_cols` are provided, it identifies all categorical columns in the DataFrame.\n",
    "- **User Confirmation:** It prompts the user to confirm if they want to create interaction features.\n",
    "- **Interaction Method Selection:** If the user chooses to proceed, it offers two options:\n",
    "    1. **Create All Pairwise Interactions:** This creates interaction features for all unique combinations of categorical columns.\n",
    "    2. **Create Interactions for Specific Columns:** This allows the user to select specific categorical columns for interaction feature generation.\n",
    "- **Feature Creation:** Based on the user's choice, it creates new features in the DataFrame by combining category combinations from the selected columns with underscores (e.g., \"column1_x_column2\").\n",
    "- **Informative Messages:** It provides clear messages throughout the process, explaining the purpose, choices available, and actions taken based on user input.\n",
    "- **Output:** It returns the modified DataFrame with additional interaction features (if created).\n",
    "\n",
    "Overall, this function simplifies interaction feature creation by guiding the user through the process and offering flexibility in choosing the level of interaction desired. It enhances user control and understanding during data preprocessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Binning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_bins(data, continuous_cols=None, n_bins=5):\n",
    "  \"\"\"\n",
    "  Creates bins (intervals) for continuous features in a DataFrame.\n",
    "\n",
    "  Args:\n",
    "      data (pandas.DataFrame): The DataFrame containing the data.\n",
    "      continuous_cols (list, optional): A list of column names to bin. If None, all continuous columns will be considered. Defaults to None.\n",
    "      n_bins (int, optional): The number of bins to create for each feature. Defaults to 5.\n",
    "\n",
    "  Returns:\n",
    "      pandas.DataFrame: The DataFrame with new bin features (categorical).\n",
    "  \"\"\"\n",
    "\n",
    "  if continuous_cols is None:\n",
    "    continuous_cols = [col for col in data.columns if data[col].dtype in ['float64', 'int64']]\n",
    "\n",
    "  if not continuous_cols:\n",
    "    print(\"No continuous features found in the data. Skipping binning.\")\n",
    "    return data\n",
    "\n",
    "  # Get user confirmation to proceed\n",
    "  action = input(\"Do you want to create bins for continuous features (y/n)? \").lower()\n",
    "  if action != \"y\":\n",
    "    print(\"Skipping binning.\")\n",
    "    return data\n",
    "\n",
    "  # Allow user to choose specific columns or bin all continuous features\n",
    "  while True:\n",
    "    choice = input(\"Choose binning method (all/specific): \").lower()\n",
    "    if choice in [\"all\", \"specific\"]:\n",
    "      break\n",
    "    else:\n",
    "      print(\"Invalid choice. Please choose 'all' or 'specific'.\")\n",
    "\n",
    "  if choice == \"all\":\n",
    "    # Bin all continuous features\n",
    "    for col in continuous_cols:\n",
    "      bins = pd.cut(data[col], bins=n_bins, labels=False) + 1  # Add 1 for informative bin names\n",
    "      data[f\"binned_{col}\"] = bins.astype(\"category\")\n",
    "      print(f\"Created bins for feature '{col}'.\")\n",
    "\n",
    "  else:\n",
    "    # Prompt user to choose specific columns for binning\n",
    "    selected_cols = []\n",
    "    while True:\n",
    "      col_name = input(\"Enter a continuous feature name (or 'done' to finish): \").lower()\n",
    "      if col_name == \"done\":\n",
    "        if not selected_cols:\n",
    "          print(\"No columns selected. Skipping binning.\")\n",
    "        else:\n",
    "          for col in selected_cols:\n",
    "            bins = pd.cut(data[col], bins=n_bins, labels=False) + 1  # Add 1 for informative bin names\n",
    "            data[f\"binned_{col}\"] = bins.astype(\"category\")\n",
    "            print(f\"Created bins for feature '{col}'.\")\n",
    "        break\n",
    "      elif col_name in continuous_cols:\n",
    "        selected_cols.append(col_name)\n",
    "        print(f\"Feature '{col_name}' added for binning.\")\n",
    "      else:\n",
    "        print(f\"Invalid column name: '{col_name}'. Please choose from continuous features.\")\n",
    "\n",
    "  return data\n",
    "\n",
    "create_feature_bins(data.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `create_feature_bins` function offers an interactive approach to creating bins (intervals) for continuous features in a DataFrame. Here's a breakdown of its functionalities:\n",
    "\n",
    "- **Input:** It takes a DataFrame (`data`), an optional list of continuous column names (`continuous_cols`), and the desired number of bins per feature (`n_bins`).\n",
    "- **Continuous Feature Identification:** If no `continuous_cols` are provided, it identifies all continuous features (numeric data types).\n",
    "- **User Confirmation:** It prompts the user to confirm if they want to create bins for continuous features.\n",
    "- **Binning Method Selection:** If the user chooses to proceed, it allows them to choose between:\n",
    "   1. **Binning All Continuous Features:** This creates bins for all identified continuous features.\n",
    "   2. **Binning Specific Features:** This allows the user to select specific continuous features for binning.\n",
    "- **Feature Selection for Binning:** If specific features are chosen, it guides the user through selecting features for binning.  \n",
    "- **Bin Creation:** Based on the chosen method and user selection, it creates new categorical features in the DataFrame named \"binned_<original_feature_name>\". Each new feature represents the bin (interval) a data point falls into for the corresponding continuous feature.\n",
    "- **Informative Messages:** It provides clear messages throughout the process, explaining the purpose, choices available, and actions taken based on user input.\n",
    "- **Output:** It returns the modified DataFrame with additional binned features (categorical).\n",
    "\n",
    "Overall, this function simplifies feature binning by providing a user-friendly interface and allowing for flexibility in selecting features and binning approach. It empowers users to participate in the data preprocessing step and potentially improve the model's ability to capture non-linear relationships in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp  # Symbolic math library (optional)\n",
    "\n",
    "def create_custom_features(data):\n",
    "  \"\"\"\n",
    "  Allows users to define and create custom features from existing features.\n",
    "\n",
    "  Args:\n",
    "      data (pandas.DataFrame): The DataFrame containing the data.\n",
    "\n",
    "  Returns:\n",
    "      pandas.DataFrame: The DataFrame with additional custom features.\n",
    "  \"\"\"\n",
    "\n",
    "  print(\"** Feature Creation Options:\")\n",
    "  print(\"- Define a new feature using existing features with mathematical expressions.\")\n",
    "  print(\"- Create interaction features from categorical columns.\")  # Reference existing function\n",
    "\n",
    "  while True:\n",
    "    choice = input(\"Choose a feature creation method (expression/interaction/none): \").lower()\n",
    "    if choice in [\"expression\", \"interaction\", \"none\"]:\n",
    "      break\n",
    "    else:\n",
    "      print(\"Invalid choice. Please choose 'expression', 'interaction', or 'none'.\")\n",
    "\n",
    "  if choice == \"expression\":\n",
    "    # Feature creation using expressions\n",
    "    while True:\n",
    "      expression = input(\"Enter a mathematical expression using existing feature names (or 'done' to finish): \")\n",
    "      if expression == \"done\":\n",
    "        break\n",
    "\n",
    "      # Validate expression using symbolic math library (optional)\n",
    "      try:\n",
    "        sp.sympify(expression)  # Raises an error for invalid expressions (optional)\n",
    "      except (TypeError, NameError):\n",
    "        print(\"Invalid expression. Please use existing feature names and basic mathematical operators (+, -, *, /).\")\n",
    "        continue\n",
    "\n",
    "      # Create and add the new feature\n",
    "      new_feature_name = input(\"Enter a name for the new feature: \")\n",
    "      try:\n",
    "        data[new_feature_name] = eval(expression)  # Evaluate the expression on the DataFrame\n",
    "        print(f\"Created new feature: '{new_feature_name}'\")\n",
    "        break  # Exit the loop if expression is valid\n",
    "      except (NameError, SyntaxError):\n",
    "        print(\"Error evaluating expression. Please check for typos or invalid syntax.\")\n",
    "\n",
    "  elif choice == \"interaction\":\n",
    "    # Call the existing create_interaction_features function (assuming it's defined)\n",
    "    data = create_interaction_features(data.copy())  # Avoid modifying original data\n",
    "\n",
    "  else:\n",
    "    print(\"Skipping custom feature creation.\")\n",
    "\n",
    "  return data\n",
    "\n",
    "create_custom_features(data.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `create_custom_features` function provides an interactive interface for users to define and create custom features from existing features in a DataFrame. Here's a breakdown of its functionalities:\n",
    "\n",
    "- **Input:** It takes a DataFrame (`data`) containing the features to potentially use for custom feature creation.\n",
    "- **Feature Creation Options:** It presents two main approaches for creating custom features:\n",
    "   1. **Expression-based Feature Creation:** Users can define a new feature using mathematical expressions involving existing feature names and basic operators (+, -, *, /).\n",
    "   2. **Interaction Feature Creation (Optional):** It offers the option to leverage the `create_interaction_features` function (assuming it's defined elsewhere) to create interaction features from categorical columns.\n",
    "- **User Choice:** The user selects their preferred method for creating custom features.\n",
    "- **Expression Definition (if chosen):**\n",
    "   - It guides the user through defining a mathematical expression.\n",
    "   - (Optional) It can perform basic validation on the expression using the `sympy` library (commented out by default) to catch syntax errors or typos.\n",
    "   - It prompts the user for a name for the newly created feature.\n",
    "   - It evaluates the user-defined expression on the DataFrame and adds the result as a new feature.\n",
    "- **Interaction Feature Creation (if chosen):**\n",
    "   - It calls the existing `create_interaction_features` function (assuming it's defined) to handle interaction feature creation. This avoids modifying the original DataFrame unnecessarily.  \n",
    "- **Output:** It returns the modified DataFrame with any newly created custom features.\n",
    "\n",
    "Overall, this function empowers users to participate in feature engineering by defining new features based on their domain knowledge and understanding of the data. It offers flexibility in the approach and provides some guidance for expression-based feature creation. Remember to exercise caution with expression evaluation and potentially implement additional validation if needed for your specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Label Encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one hot encoding\n",
    "\n",
    "label encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Handling Class Imbalance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Selection**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
