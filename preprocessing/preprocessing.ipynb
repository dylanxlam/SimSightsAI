{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  # for nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Type Conversion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Types\n",
    "data_types = data.dtypes\n",
    "print(\"Data Types for Each Column in Your Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def convert_data_types(data):\n",
    "  \"\"\"\n",
    "  Prints data types for each column and allows user to change them.\n",
    "\n",
    "  Args:\n",
    "      data (pandas.DataFrame): The DataFrame containing the data.\n",
    "\n",
    "  Returns:\n",
    "      pandas.DataFrame: The DataFrame potentially with changed data types.\n",
    "  \"\"\"\n",
    "\n",
    "  # Explain data types in a dictionary for easy reference\n",
    "  dtype_explanations = {\n",
    "      'int64': \"Integer (whole numbers, positive or negative)\",\n",
    "      'float64': \"Decimal number\",\n",
    "      'object': \"Text data (strings)\",\n",
    "      'category': \"Categorical data (limited set of options)\",\n",
    "      'datetime64[ns]': \"Date and time\",\n",
    "      'bool': \"Boolean (True or False)\"\n",
    "  }\n",
    "\n",
    "  # Print data types with explanations\n",
    "  for col, dtype in data_types.items():\n",
    "    print(f\"- {col}: {dtype} ({dtype_explanations.get(dtype, 'Unknown')})\")\n",
    "\n",
    "  # Prompt user for data type changes\n",
    "  change_dtypes = input(\"Would you like to change any data types (y/n)? \").lower()\n",
    "  if change_dtypes == \"y\":\n",
    "    while True:\n",
    "      # Ask for column and desired data type\n",
    "      col_to_change = input(\"Enter the column name to change the data type: \").lower()\n",
    "      new_dtype = input(\"Enter the desired new data type (int, float, object, etc.): \").lower()\n",
    "\n",
    "      # Check if column exists and new data type is valid\n",
    "      if col_to_change in data.columns and new_dtype in dtype_explanations.keys():\n",
    "        try:\n",
    "          # Attempt conversion (handles potential errors)\n",
    "          data[col_to_change] = data[col_to_change].astype(new_dtype)\n",
    "          print(f\"Data type for '{col_to_change}' changed to {new_dtype}.\")\n",
    "          # **Modified break logic:**\n",
    "          break_loop = input(\"Do you want to convert another column (y/n)? \").lower()\n",
    "          if break_loop != \"y\":\n",
    "            break\n",
    "        except (ValueError, TypeError) as e:\n",
    "          print(f\"Error converting '{col_to_change}' to {new_dtype}: {e}\")\n",
    "          # **Prompt to continue after error**\n",
    "          continue_loop = input(\"Would you like to try converting another column (y/n)? \").lower()\n",
    "          if continue_loop != \"y\":\n",
    "            break\n",
    "      else:\n",
    "        print(f\"Invalid column name or data type. Please try again.\")\n",
    "\n",
    "  return data\n",
    "\n",
    "# Example usage (assuming data is your DataFrame)\n",
    "data = convert_data_types(data.copy())  # Avoid modifying original data\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focus: Data cleaning addresses inconsistencies, errors, and missing values within the data itself.\n",
    "\n",
    "Data Type Conversion: In this context, converting data types is often a cleaning step when the data type is incorrect or incompatible with how the data should be represented.\n",
    "Examples:\n",
    "- Inconsistent date formats (text to datetime).\n",
    "- Text values in numerical columns (text to numerical).\n",
    "- Incorrect data types due to import issues (e.g., strings instead of integers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dealing With Normality and Skewness**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most efficient way to assess normality and skewness in your data columns depends on a few factors:\n",
    "\n",
    "**1. Number of Columns:**\n",
    "\n",
    "* **Few Columns:** If you have a small number of columns (less than 10), visual inspection using histograms and QQ plots might be the most efficient approach. These techniques are easy to understand and interpret, providing a quick grasp of the data distribution.\n",
    "\n",
    "* **Many Columns:** With a large number of columns (more than 10), visual inspection becomes cumbersome. Here, statistical tests like the Shapiro-Wilk test can be more efficient. You can calculate the test statistic and p-value for each column to identify potential deviations from normality. A threshold for the p-value (e.g., 0.05) can be used to decide if the data is likely non-normal.\n",
    "\n",
    "**2. Desired Level of Detail:**\n",
    "\n",
    "* **Basic Assessment:** If you just need a quick indication of normality or skewness, histograms and statistical tests with p-values provide a sufficient level of detail.\n",
    "\n",
    "* **Detailed Analysis:** For a more in-depth analysis, you can combine both approaches. Start with histograms and QQ plots to get a visual sense of the distribution, and then follow up with statistical tests to confirm your observations or explore borderline cases with p-values close to the chosen threshold.\n",
    "\n",
    "Here's a breakdown of the efficiency considerations:\n",
    "\n",
    "| Method | Efficiency for Few Columns | Efficiency for Many Columns | Level of Detail |\n",
    "|---|---|---|---|\n",
    "| Histograms & QQ Plots | High (easy to interpret visually) | Low (time-consuming for many columns) | High (visual assessment of shape) |\n",
    "| Statistical Tests | Low (requires calculations) | High (efficient for many columns) | Moderate (p-value indicates normality likelihood) |\n",
    "\n",
    "**Combined Approach:**\n",
    "\n",
    "In practice, a combination of visual inspection and statistical tests often offers the best balance between efficiency and detail. Start with histograms and QQ plots for a quick overview, then use statistical tests for more rigorous confirmation, especially when dealing with many columns.\n",
    "\n",
    "Here are some additional factors to consider:\n",
    "\n",
    "* **Computational Resources:** If computational resources are limited, visual methods might be preferred. Statistical tests, especially for large datasets, can require more processing power.\n",
    "* **Domain Knowledge:** If you have domain knowledge about the data, you might have an initial expectation about the normality of certain features. This can guide your choice of method, focusing on tests for features where normality is critical for your analysis.\n",
    "\n",
    "Ultimately, the most efficient approach depends on your specific needs and the size of your dataset. Combining visual and statistical methods often provides a comprehensive and efficient way to assess normality and skewness in your data columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting Skewed Data\n",
    "\n",
    "def identify_skewed_data(data):\n",
    "    \"\"\"\n",
    "    Identifies skewed features in a DataFrame and prompts for correction.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The DataFrame containing the data.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The original DataFrame (potentially with user-guided transformations).\n",
    "    \"\"\"\n",
    "    numerical_cols = data.select_dtypes(include=[np.number])\n",
    "    skewed_cols = []  # List to store column names with skewness\n",
    "\n",
    "    # Threshold for skewness (adjust as needed)\n",
    "    skewness_threshold = 0.5\n",
    "\n",
    "    for col in numerical_cols:\n",
    "        # Calculate skewness\n",
    "        skew = data[col].skew()\n",
    "\n",
    "    if abs(skew) > skewness_threshold:\n",
    "        skewed_cols.append(col)\n",
    "        print(f\"Column '{col}' appears skewed (skewness: {skew:.2f}).\")\n",
    "        action = input(\"Do you want to address the skewness (y/n)? \").lower()\n",
    "        if action == \"y\":\n",
    "            # User chooses to address skewness\n",
    "            fix_method = input(\"Choose a correction method (log/sqrt/none): \").lower()\n",
    "            if fix_method in [\"log\", \"sqrt\"]:\n",
    "                # Apply transformation (log or sqrt)\n",
    "                if fix_method == \"log\":\n",
    "                    data[col] = np.log(data[col] + 1)  # Avoid log(0) errors by adding 1\n",
    "                    print(f\"Applied log transformation to column '{col}'.\")\n",
    "                else:\n",
    "                    data[col] = np.sqrt(data[col])\n",
    "                    print(f\"Applied square root transformation to column '{col}'.\")\n",
    "            else:\n",
    "                print(f\"Skewness in '{col}' remains unaddressed.\")\n",
    "        else:\n",
    "            print(f\"Skewness in '{col}' remains unaddressed.\")\n",
    "\n",
    "    if not skewed_cols:\n",
    "        print(\"No significant skewness detected in numerical columns.\")\n",
    "\n",
    "    return data\n",
    "\n",
    "    # Example usage\n",
    "    data = pd.DataFrame({ 'col1': [1, 2, 3, 4, 5], 'col2': [10, 100, 1000, 10000, 100000]})\n",
    "    data = identify_skewed_data(data.copy())  # Avoid modifying original data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "1. Function Definition: The identify_skewed_data function takes a DataFrame (data) as input.\n",
    "2. Numerical Columns: It selects numerical columns using select_dtypes.\n",
    "3. Skewness Threshold: A threshold for skewness (skewness_threshold) is defined (adjustable based on your needs).\n",
    "4. Iterating Through Columns: The code loops through each numerical column.\n",
    "5. Skewness Calculation: It calculates the skewness using data[col].skew().\n",
    "6. Identifying Skewed Columns: If the absolute value of skewness is greater than the threshold, the column name is added to the skewed_cols list, and a message is printed informing the user about the skewness value.\n",
    "7. User Prompt: The user is then prompted to address the skewness (yes/no).\n",
    "8. User Choice: If the user chooses \"y\", another prompt asks for a correction method (log, sqrt, or none).\n",
    "9. Transformation Options: Based on the chosen method (log or sqrt), the data in that column is transformed using np.log or np.sqrt (with safeguards to avoid log(0) errors). A message confirms the applied transformation.\n",
    "10. No Transformation: If the user chooses \"n\" or an invalid method, a message indicates that the skewness remains unaddressed.\n",
    "11. No Skewness Detected: If no columns have significant skewness, a message informs the user.\n",
    "12. Data Return: The original DataFrame is returned (potentially with user-guided transformations on skewed columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing/Scaling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization and scaling are closely related data preprocessing techniques used in machine learning, but they have subtle differences:\n",
    "\n",
    "**Normalization:**\n",
    "\n",
    "* **Goal:** Normalize data features to a specific range (often 0 to 1 or -1 to 1). This ensures all features contribute equally during model training and can improve the convergence and stability of some algorithms. \n",
    "* **Methods:** Common normalization techniques include:\n",
    "    * **Min-Max Scaling:** Scales each feature to a range between a minimum value (usually 0) and a maximum value (usually 1).\n",
    "    * **Standardization (Z-score):** Subtracts the mean of each feature from its values and then divides by the standard deviation. This results in a standard normal distribution with a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "**Scaling:**\n",
    "\n",
    "* **Goal:** Scale data features to have a similar range or variance. This can be helpful for algorithms that are sensitive to the scale of the data. While normalization achieves a specific range, scaling can involve transformations to a broader range depending on the chosen method.\n",
    "* **Methods:** Scaling encompasses various techniques, including normalization (min-max scaling and standardization) as well as:\n",
    "    * **Robust Scaling:** Similar to standardization but uses the median and interquartile range (IQR) to be less sensitive to outliers.\n",
    "\n",
    "**Here's an analogy:**\n",
    "\n",
    "Imagine ingredients for a recipe.\n",
    "\n",
    "* **Normalization:** This is like measuring all ingredients in teaspoons or grams (a specific unit system).\n",
    "* **Scaling:** This is like ensuring all ingredients are in similar quantities, even if not using the same units (e.g., adjusting cup measurements to be closer to the amount of a teaspoon measurement used in another ingredient).\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "* **Normalization is a specific type of scaling:**  Normalization techniques (min-max scaling and standardization) fall under the broader category of scaling.\n",
    "* **Scaling can be more general:** Scaling can encompass methods beyond normalization, like robust scaling, which might be preferable in some scenarios.\n",
    "* **Focus:** Normalization emphasizes bringing features to a specific range, while scaling focuses on making features have similar scales or variances.\n",
    "\n",
    "**In practice, the terms \"normalization\" and \"scaling\" are sometimes used interchangeably, especially when referring to min-max scaling or standardization.** However, it's important to understand the nuances to choose the most appropriate technique for your data and modeling task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Label Encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Handling Class Imbalance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Selection**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
