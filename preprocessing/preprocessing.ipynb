{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  # for nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Type Conversion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Types\n",
    "data_types = data.dtypes\n",
    "print(\"Data Types for Each Column in Your Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def convert_data_types(data):\n",
    "  \"\"\"\n",
    "  Prints data types for each column and allows user to change them.\n",
    "\n",
    "  Args:\n",
    "      data (pandas.DataFrame): The DataFrame containing the data.\n",
    "\n",
    "  Returns:\n",
    "      pandas.DataFrame: The DataFrame potentially with changed data types.\n",
    "  \"\"\"\n",
    "\n",
    "  # Explain data types in a dictionary for easy reference\n",
    "  dtype_explanations = {\n",
    "      'int64': \"Integer (whole numbers, positive or negative)\",\n",
    "      'float64': \"Decimal number\",\n",
    "      'object': \"Text data (strings)\",\n",
    "      'category': \"Categorical data (limited set of options)\",\n",
    "      'datetime64[ns]': \"Date and time\",\n",
    "      'bool': \"Boolean (True or False)\"\n",
    "  }\n",
    "\n",
    "  # Print data types with explanations\n",
    "  for col, dtype in data_types.items():\n",
    "    print(f\"- {col}: {dtype} ({dtype_explanations.get(dtype, 'Unknown')})\")\n",
    "\n",
    "  # Prompt user for data type changes\n",
    "  change_dtypes = input(\"Would you like to change any data types (y/n)? \").lower()\n",
    "  if change_dtypes == \"y\":\n",
    "    while True:\n",
    "      # Ask for column and desired data type\n",
    "      col_to_change = input(\"Enter the column name to change the data type: \").lower()\n",
    "      new_dtype = input(\"Enter the desired new data type (int, float, object, etc.): \").lower()\n",
    "\n",
    "      # Check if column exists and new data type is valid\n",
    "      if col_to_change in data.columns and new_dtype in dtype_explanations.keys():\n",
    "        try:\n",
    "          # Attempt conversion (handles potential errors)\n",
    "          data[col_to_change] = data[col_to_change].astype(new_dtype)\n",
    "          print(f\"Data type for '{col_to_change}' changed to {new_dtype}.\")\n",
    "          # **Modified break logic:**\n",
    "          break_loop = input(\"Do you want to convert another column (y/n)? \").lower()\n",
    "          if break_loop != \"y\":\n",
    "            break\n",
    "        except (ValueError, TypeError) as e:\n",
    "          print(f\"Error converting '{col_to_change}' to {new_dtype}: {e}\")\n",
    "          # **Prompt to continue after error**\n",
    "          continue_loop = input(\"Would you like to try converting another column (y/n)? \").lower()\n",
    "          if continue_loop != \"y\":\n",
    "            break\n",
    "      else:\n",
    "        print(f\"Invalid column name or data type. Please try again.\")\n",
    "\n",
    "  return data\n",
    "\n",
    "# Example usage \n",
    "data = convert_data_types(data.copy())  # Avoid modifying original data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focus: Data cleaning addresses inconsistencies, errors, and missing values within the data itself.\n",
    "\n",
    "Data Type Conversion: In this context, converting data types is often a cleaning step when the data type is incorrect or incompatible with how the data should be represented.\n",
    "Examples:\n",
    "- Inconsistent date formats (text to datetime).\n",
    "- Text values in numerical columns (text to numerical).\n",
    "- Incorrect data types due to import issues (e.g., strings instead of integers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dealing With Normality and Skewness**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most efficient way to assess normality and skewness in your data columns depends on a few factors:\n",
    "\n",
    "**1. Number of Columns:**\n",
    "\n",
    "* **Few Columns:** `(still need implementation)` If you have a small number of columns (less than 10), visual inspection using histograms and QQ plots might be the most efficient approach. These techniques are easy to understand and interpret, providing a quick grasp of the data distribution.\n",
    "\n",
    "* **Many Columns:** With a large number of columns (more than 10), visual inspection becomes cumbersome. Here, statistical tests like the Shapiro-Wilk test can be more efficient. You can calculate the test statistic and p-value for each column to identify potential deviations from normality. A threshold for the p-value (e.g., 0.05) can be used to decide if the data is likely non-normal.\n",
    "\n",
    "**2. Desired Level of Detail:**\n",
    "\n",
    "* **Basic Assessment:** If you just need a quick indication of normality or skewness, histograms and statistical tests with p-values provide a sufficient level of detail.\n",
    "\n",
    "* **Detailed Analysis:** For a more in-depth analysis, you can combine both approaches. Start with histograms and QQ plots to get a visual sense of the distribution, and then follow up with statistical tests to confirm your observations or explore borderline cases with p-values close to the chosen threshold.\n",
    "\n",
    "Here's a breakdown of the efficiency considerations:\n",
    "\n",
    "| Method | Efficiency for Few Columns | Efficiency for Many Columns | Level of Detail |\n",
    "|---|---|---|---|\n",
    "| Histograms & QQ Plots | High (easy to interpret visually) | Low (time-consuming for many columns) | High (visual assessment of shape) |\n",
    "| Statistical Tests | Low (requires calculations) | High (efficient for many columns) | Moderate (p-value indicates normality likelihood) |\n",
    "\n",
    "**Combined Approach:**\n",
    "\n",
    "In practice, a combination of visual inspection and statistical tests often offers the best balance between efficiency and detail. Start with histograms and QQ plots for a quick overview, then use statistical tests for more rigorous confirmation, especially when dealing with many columns.\n",
    "\n",
    "Here are some additional factors to consider:\n",
    "\n",
    "* **Computational Resources:** If computational resources are limited, visual methods might be preferred. Statistical tests, especially for large datasets, can require more processing power.\n",
    "* **Domain Knowledge:** If you have domain knowledge about the data, you might have an initial expectation about the normality of certain features. This can guide your choice of method, focusing on tests for features where normality is critical for your analysis.\n",
    "\n",
    "Ultimately, the most efficient approach depends on your specific needs and the size of your dataset. Combining visual and statistical methods often provides a comprehensive and efficient way to assess normality and skewness in your data columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Normalizing/Scaling Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "def scale(data):\n",
    "  \"\"\"\n",
    "  Identifies skewed features, suggests corrections, and performs scaling/normalization.\n",
    "\n",
    "  Args:\n",
    "      data (pandas.DataFrame): The DataFrame containing the data.\n",
    "\n",
    "  Returns:\n",
    "      pandas.DataFrame: The transformed DataFrame with addressed skewness and scaling/normalization.\n",
    "  \"\"\"\n",
    "  numerical_cols = data.select_dtypes(include=[np.number])\n",
    "  skewed_cols = []  # List to store column names with skewness\n",
    "\n",
    "  # Threshold for skewness (adjust as needed)\n",
    "  skewness_threshold = 0.5\n",
    "\n",
    "  for col in numerical_cols:\n",
    "    # Calculate skewness\n",
    "    skew = data[col].skew()\n",
    "    if abs(skew) > skewness_threshold:\n",
    "      skewed_cols.append(col)\n",
    "      print(f\"Column '{col}' appears skewed (skewness: {skew:.2f}).\")\n",
    "\n",
    "\n",
    "      # Inform decision-making\n",
    "      print(\"Here's a brief explanation of the available correction methods:\")\n",
    "      print(\"  - Log transformation (log(x + 1)): This method is often effective for right-skewed data (where values are concentrated on the left side of the distribution).\")\n",
    "      print(\"    It compresses the larger values and stretches the smaller ones, aiming for a more symmetrical distribution.\")\n",
    "      print(\"  - Square root transformation (sqrt(x)): This method can be helpful for moderately skewed data, positive-valued features, or data with a large number of zeros.\")\n",
    "      print(\"    It reduces the influence of extreme values and can bring the distribution closer to normality.\")\n",
    "      print(\"**Please consider the characteristics of your skewed feature(s) when making your choice.**\")\n",
    "      print(\"If you're unsure, you can experiment with both methods and compare the results visually (e.g., using histograms) to see which one normalizes the data more effectively for your specific case.\")\n",
    "\n",
    "      # User prompt for addressing skewness\n",
    "      action = input(\"Do you want to address the skewness (y/n)? \").lower()\n",
    "      if action == \"y\":\n",
    "        \n",
    "\n",
    "        # User chooses to address skewness\n",
    "        while True:  # Loop until a valid choice is made\n",
    "          fix_method = input(\"Choose a correction method (log/sqrt/none): \").lower()\n",
    "          if fix_method in [\"log\", \"sqrt\"]:\n",
    "            # Apply transformation (log or sqrt)\n",
    "            if fix_method == \"log\":\n",
    "              data[col] = np.log(data[col] + 1)  # Avoid log(0) errors by adding 1\n",
    "              print(f\"Applied log transformation to column '{col}'.\")\n",
    "            else:\n",
    "              data[col] = np.sqrt(data[col])\n",
    "              print(f\"Applied square root transformation to column '{col}'.\")\n",
    "            break  # Exit the loop if a valid choice is made\n",
    "          else:\n",
    "            print(\"Invalid choice. Please choose 'log', 'sqrt', or 'none'.\")\n",
    "\n",
    "      else:\n",
    "        print(f\"Skewness in '{col}' remains unaddressed.\")\n",
    "    \n",
    "    if not skewed_cols:\n",
    "      print(\"No significant skewness detected in numerical columns.\")\n",
    "\n",
    "  # User prompt for scaling/normalization (if applicable)\n",
    "  if len(numerical_cols) > 0:\n",
    "\n",
    "    print(\"Here's a brief explanation of the available scaling/normalization methods:\")\n",
    "    print(\"  - Standard scaling: This method transforms features by subtracting the mean and dividing by the standard deviation.\")\n",
    "    print(\"    This results in features centered around zero with a standard deviation of 1.\")\n",
    "    print(\"    It's suitable for algorithms that assume a normal distribution of features (e.g., Logistic Regression, Support Vector Machines).\")\n",
    "    print(\"  - Min-max scaling: This method scales each feature to a specific range, typically between 0 and 1.\")\n",
    "    print(\"    It achieves this by subtracting the minimum value and then dividing by the difference between the maximum and minimum values in the feature.\")\n",
    "    print(\"    This can be useful for algorithms that are sensitive to the scale of features (e.g., K-Nearest Neighbors).\")\n",
    "    print(\"**Choosing the right method depends on your data and the algorithm you're using.**\")\n",
    "    print(\"  - If you're unsure about the underlying distribution of your data, standard scaling might be a safer choice as it doesn't make assumptions about normality.\")\n",
    "    print(\"  - If your algorithm is sensitive to feature scales and doesn't assume normality, min-max scaling might be preferable.\")\n",
    "    print(\"Consider the characteristics of your data and algorithm when making your decision. You can also experiment with both methods\")\n",
    "    print(\"and compare the results using model performance metrics to see which one works best for your specific case.\")\n",
    "\n",
    "    action = input(\"Do you want to scale or normalize the numerical features (y/n)? \").lower()\n",
    "    if action == \"y\":\n",
    "\n",
    "      while True:  # Loop until a valid choice is made\n",
    "        method = input(\"Choose scaling/normalization method (standard/minmax/skip): \").lower()\n",
    "        if method in [\"standard\", \"minmax\"]:\n",
    "          if method == \"standard\":\n",
    "            scaler = StandardScaler()\n",
    "            data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "            print(f\"Applied standard scaling to numerical features.\")\n",
    "          else:\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "            print(f\"Applied min-max scaling to numerical features (range 0-1).\")\n",
    "          break  # Exit the loop if a valid choice is made\n",
    "        elif method == \"skip\":\n",
    "          print(\"Skipping scaling/normalization.\")\n",
    "          break\n",
    "        else:\n",
    "          print(\"Invalid choice. Please choose 'standard', 'minmax', or 'skip'.\")\n",
    "\n",
    "\n",
    "  if not skewed_cols:\n",
    "    print(\"No significant skewness detected in numerical columns.\")\n",
    "  return data\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "preprocessed_data = scale(data.copy())  # Operate on a copy to avoid modifying original data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scale` function offers a user-guided approach to data preprocessing, addressing both skewness in numerical features and optional scaling/normalization. Here's a breakdown of its functionalities:\n",
    "\n",
    "1. **Skewness Identification:**\n",
    "   - It identifies features with significant skewness (asymmetry in the distribution) based on a user-defined threshold.\n",
    "   - For skewed features, it informs the user about the skewness value and prompts them to address it.\n",
    "\n",
    "2. **Skewness Correction (Optional):**\n",
    "   - If the user chooses to address skewness, it offers options for log or square root transformation to potentially reduce skewness.\n",
    "   - The chosen transformation is applied to the specific feature(s).\n",
    "\n",
    "3. **Scaling/Normalization (Optional):**\n",
    "   - After addressing skewness (or if no skewness is found), it prompts the user to decide if they want to scale or normalize the numerical features.\n",
    "   - If the user chooses to proceed, it offers options for standard scaling (centering and scaling features to have zero mean and unit variance) or min-max scaling (scaling features to a specific range, typically 0-1).\n",
    "   - The chosen scaling/normalization method is applied using scikit-learn's `StandardScaler` or `MinMaxScaler` to transform the numerical features.\n",
    "\n",
    "\n",
    "Overall, this function simplifies data preprocessing by guiding the user through common steps like handling skewness and applying scaling/normalization techniques. It provides flexibility and explanations, making it easier to understand and customize the preprocessing process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Creating Interaction Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interaction_features(data, categorical_cols=None):\n",
    "  \"\"\"\n",
    "  Creates interaction features from categorical columns in a DataFrame.\n",
    "\n",
    "  Args:\n",
    "      data (pandas.DataFrame): The DataFrame containing the data.\n",
    "      categorical_cols (list, optional): A list of column names to consider for interaction features. If None, all categorical columns will be used. Defaults to None.\n",
    "\n",
    "  Returns:\n",
    "      pandas.DataFrame: The DataFrame with additional interaction features.\n",
    "  \"\"\"\n",
    "\n",
    "  if categorical_cols is None:\n",
    "    categorical_cols = [col for col in data.columns if data[col].dtype == 'category']\n",
    "\n",
    "  if not categorical_cols:\n",
    "    print(\"No categorical columns found in the data. Skipping interaction feature creation.\")\n",
    "    return data\n",
    "\n",
    "  # Display recommendations before prompting user\n",
    "  print(\"** Recommendations for Interaction Features:**\")\n",
    "  print(\"- Interaction features can capture complex relationships, potentially improving model performance.\")\n",
    "  print(\"- However, creating all possible interactions can lead to data sparsity and longer training times.\")\n",
    "  print(\"- Consider your domain knowledge to prioritize specific interactions.\")\n",
    "  print(\"- Start with a smaller set and use feature selection techniques for better interpretability.\")\n",
    "\n",
    "\n",
    "  # Get user confirmation to proceed\n",
    "  action = input(\"Do you want to create interaction features from categorical columns (y/n)? \").lower()\n",
    "  if action != \"y\":\n",
    "    print(\"Skipping interaction feature creation.\")\n",
    "    return data\n",
    "\n",
    "  # Prompt user to choose specific columns or create all possible interactions\n",
    "  while True:\n",
    "    choice = input(\"Choose interaction feature creation method (all/specific): \").lower()\n",
    "    if choice in [\"all\", \"specific\"]:\n",
    "      break\n",
    "    else:\n",
    "      print(\"Invalid choice. Please choose 'all' or 'specific'.\")\n",
    "\n",
    "  if choice == \"all\":\n",
    "    # Create all pairwise interaction features\n",
    "    for col1 in categorical_cols:\n",
    "      for col2 in categorical_cols:\n",
    "        if col1 != col2:\n",
    "          data[f\"{col1}_x_{col2}\"] = data[col1].astype(str) + \"_\" + data[col2].astype(str)\n",
    "    print(\"Created all possible pairwise interaction features.\")\n",
    "\n",
    "  else:\n",
    "    # Prompt user to choose specific columns for interaction\n",
    "    selected_cols = []\n",
    "    while True:\n",
    "      col_name = input(\"Enter a categorical column name (or 'done' to finish): \").lower()\n",
    "      if col_name == \"done\":\n",
    "        if not selected_cols:\n",
    "          print(\"No columns selected. Skipping interaction feature creation.\")\n",
    "        else:\n",
    "          for col1 in selected_cols:\n",
    "            for col2 in selected_cols:\n",
    "              if col1 != col2:\n",
    "                data[f\"{col1}_x_{col2}\"] = data[col1].astype(str) + \"_\" + data[col2].astype(str)\n",
    "          print(f\"Created interaction features for selected columns: {', '.join(selected_cols)}\")\n",
    "        break\n",
    "      elif col_name in categorical_cols:\n",
    "        selected_cols.append(col_name)\n",
    "        print(f\"Column '{col_name}' added for interaction features.\")\n",
    "      else:\n",
    "        print(f\"Invalid column name: '{col_name}'. Please choose from categorical columns.\")\n",
    "\n",
    "  return data\n",
    "\n",
    "create_interaction_features(data.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `create_interaction_features` function offers a user-guided approach to creating interaction features from categorical columns in a DataFrame. Here's a breakdown of its functionalities:\n",
    "\n",
    "- **Input:** It takes a DataFrame (`data`) and an optional list of categorical column names (`categorical_cols`).\n",
    "- **Categorical Column Identification:** If no `categorical_cols` are provided, it identifies all categorical columns in the DataFrame.\n",
    "- **User Confirmation:** It prompts the user to confirm if they want to create interaction features.\n",
    "- **Interaction Method Selection:** If the user chooses to proceed, it offers two options:\n",
    "    1. **Create All Pairwise Interactions:** This creates interaction features for all unique combinations of categorical columns.\n",
    "    2. **Create Interactions for Specific Columns:** This allows the user to select specific categorical columns for interaction feature generation.\n",
    "- **Feature Creation:** Based on the user's choice, it creates new features in the DataFrame by combining category combinations from the selected columns with underscores (e.g., \"column1_x_column2\").\n",
    "- **Informative Messages:** It provides clear messages throughout the process, explaining the purpose, choices available, and actions taken based on user input.\n",
    "- **Output:** It returns the modified DataFrame with additional interaction features (if created).\n",
    "\n",
    "Overall, this function simplifies interaction feature creation by guiding the user through the process and offering flexibility in choosing the level of interaction desired. It enhances user control and understanding during data preprocessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Binning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_bins(data, continuous_cols=None, n_bins=5):\n",
    "  \"\"\"\n",
    "  Creates bins (intervals) for continuous features in a DataFrame.\n",
    "\n",
    "  Args:\n",
    "      data (pandas.DataFrame): The DataFrame containing the data.\n",
    "      continuous_cols (list, optional): A list of column names to bin. If None, all continuous columns will be considered. Defaults to None.\n",
    "      n_bins (int, optional): The number of bins to create for each feature. Defaults to 5.\n",
    "\n",
    "  Returns:\n",
    "      pandas.DataFrame: The DataFrame with new bin features (categorical).\n",
    "  \"\"\"\n",
    "\n",
    "  if continuous_cols is None:\n",
    "    continuous_cols = [col for col in data.columns if data[col].dtype in ['float64', 'int64']]\n",
    "\n",
    "  if not continuous_cols:\n",
    "    print(\"No continuous features found in the data. Skipping binning.\")\n",
    "    return data\n",
    "\n",
    "  # Get user confirmation to proceed\n",
    "  action = input(\"Do you want to create bins for continuous features (y/n)? \").lower()\n",
    "  if action != \"y\":\n",
    "    print(\"Skipping binning.\")\n",
    "    return data\n",
    "\n",
    "  # Allow user to choose specific columns or bin all continuous features\n",
    "  while True:\n",
    "    choice = input(\"Choose binning method (all/specific): \").lower()\n",
    "    if choice in [\"all\", \"specific\"]:\n",
    "      break\n",
    "    else:\n",
    "      print(\"Invalid choice. Please choose 'all' or 'specific'.\")\n",
    "\n",
    "  if choice == \"all\":\n",
    "    # Bin all continuous features\n",
    "    for col in continuous_cols:\n",
    "      bins = pd.cut(data[col], bins=n_bins, labels=False) + 1  # Add 1 for informative bin names\n",
    "      data[f\"binned_{col}\"] = bins.astype(\"category\")\n",
    "      print(f\"Created bins for feature '{col}'.\")\n",
    "\n",
    "  else:\n",
    "    # Prompt user to choose specific columns for binning\n",
    "    selected_cols = []\n",
    "    while True:\n",
    "      col_name = input(\"Enter a continuous feature name (or 'done' to finish): \").lower()\n",
    "      if col_name == \"done\":\n",
    "        if not selected_cols:\n",
    "          print(\"No columns selected. Skipping binning.\")\n",
    "        else:\n",
    "          for col in selected_cols:\n",
    "            bins = pd.cut(data[col], bins=n_bins, labels=False) + 1  # Add 1 for informative bin names\n",
    "            data[f\"binned_{col}\"] = bins.astype(\"category\")\n",
    "            print(f\"Created bins for feature '{col}'.\")\n",
    "        break\n",
    "      elif col_name in continuous_cols:\n",
    "        selected_cols.append(col_name)\n",
    "        print(f\"Feature '{col_name}' added for binning.\")\n",
    "      else:\n",
    "        print(f\"Invalid column name: '{col_name}'. Please choose from continuous features.\")\n",
    "\n",
    "  return data\n",
    "\n",
    "create_feature_bins(data.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `create_feature_bins` function offers an interactive approach to creating bins (intervals) for continuous features in a DataFrame. Here's a breakdown of its functionalities:\n",
    "\n",
    "- **Input:** It takes a DataFrame (`data`), an optional list of continuous column names (`continuous_cols`), and the desired number of bins per feature (`n_bins`).\n",
    "- **Continuous Feature Identification:** If no `continuous_cols` are provided, it identifies all continuous features (numeric data types).\n",
    "- **User Confirmation:** It prompts the user to confirm if they want to create bins for continuous features.\n",
    "- **Binning Method Selection:** If the user chooses to proceed, it allows them to choose between:\n",
    "   1. **Binning All Continuous Features:** This creates bins for all identified continuous features.\n",
    "   2. **Binning Specific Features:** This allows the user to select specific continuous features for binning.\n",
    "- **Feature Selection for Binning:** If specific features are chosen, it guides the user through selecting features for binning.  \n",
    "- **Bin Creation:** Based on the chosen method and user selection, it creates new categorical features in the DataFrame named \"binned_<original_feature_name>\". Each new feature represents the bin (interval) a data point falls into for the corresponding continuous feature.\n",
    "- **Informative Messages:** It provides clear messages throughout the process, explaining the purpose, choices available, and actions taken based on user input.\n",
    "- **Output:** It returns the modified DataFrame with additional binned features (categorical).\n",
    "\n",
    "Overall, this function simplifies feature binning by providing a user-friendly interface and allowing for flexibility in selecting features and binning approach. It empowers users to participate in the data preprocessing step and potentially improve the model's ability to capture non-linear relationships in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp  # Symbolic math library (optional)\n",
    "\n",
    "def create_custom_features(data):\n",
    "  \"\"\"\n",
    "  Allows users to define and create custom features from existing features.\n",
    "\n",
    "  Args:\n",
    "      data (pandas.DataFrame): The DataFrame containing the data.\n",
    "\n",
    "  Returns:\n",
    "      pandas.DataFrame: The DataFrame with additional custom features.\n",
    "  \"\"\"\n",
    "\n",
    "  print(\"** Feature Creation Options:\")\n",
    "  print(\"- Define a new feature using existing features with mathematical expressions.\")\n",
    "  print(\"- Create interaction features from categorical columns.\")  # Reference existing function\n",
    "\n",
    "  while True:\n",
    "    choice = input(\"Choose a feature creation method (expression/interaction/none): \").lower()\n",
    "    if choice in [\"expression\", \"interaction\", \"none\"]:\n",
    "      break\n",
    "    else:\n",
    "      print(\"Invalid choice. Please choose 'expression', 'interaction', or 'none'.\")\n",
    "\n",
    "  if choice == \"expression\":\n",
    "    # Feature creation using expressions\n",
    "    while True:\n",
    "      expression = input(\"Enter a mathematical expression using existing feature names (or 'done' to finish): \")\n",
    "      if expression == \"done\":\n",
    "        break\n",
    "\n",
    "      # Validate expression using symbolic math library (optional)\n",
    "      try:\n",
    "        sp.sympify(expression)  # Raises an error for invalid expressions (optional)\n",
    "      except (TypeError, NameError):\n",
    "        print(\"Invalid expression. Please use existing feature names and basic mathematical operators (+, -, *, /).\")\n",
    "        continue\n",
    "\n",
    "      # Create and add the new feature\n",
    "      new_feature_name = input(\"Enter a name for the new feature: \")\n",
    "      try:\n",
    "        data[new_feature_name] = eval(expression)  # Evaluate the expression on the DataFrame\n",
    "        print(f\"Created new feature: '{new_feature_name}'\")\n",
    "        break  # Exit the loop if expression is valid\n",
    "      except (NameError, SyntaxError):\n",
    "        print(\"Error evaluating expression. Please check for typos or invalid syntax.\")\n",
    "\n",
    "  elif choice == \"interaction\":\n",
    "    # Call the existing create_interaction_features function (assuming it's defined)\n",
    "    data = create_interaction_features(data.copy())  # Avoid modifying original data\n",
    "\n",
    "  else:\n",
    "    print(\"Skipping custom feature creation.\")\n",
    "\n",
    "  return data\n",
    "\n",
    "create_custom_features(data.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `create_custom_features` function provides an interactive interface for users to define and create custom features from existing features in a DataFrame. Here's a breakdown of its functionalities:\n",
    "\n",
    "- **Input:** It takes a DataFrame (`data`) containing the features to potentially use for custom feature creation.\n",
    "- **Feature Creation Options:** It presents two main approaches for creating custom features:\n",
    "   1. **Expression-based Feature Creation:** Users can define a new feature using mathematical expressions involving existing feature names and basic operators (+, -, *, /).\n",
    "   2. **Interaction Feature Creation (Optional):** It offers the option to leverage the `create_interaction_features` function (assuming it's defined elsewhere) to create interaction features from categorical columns.\n",
    "- **User Choice:** The user selects their preferred method for creating custom features.\n",
    "- **Expression Definition (if chosen):**\n",
    "   - It guides the user through defining a mathematical expression.\n",
    "   - (Optional) It can perform basic validation on the expression using the `sympy` library (commented out by default) to catch syntax errors or typos.\n",
    "   - It prompts the user for a name for the newly created feature.\n",
    "   - It evaluates the user-defined expression on the DataFrame and adds the result as a new feature.\n",
    "- **Interaction Feature Creation (if chosen):**\n",
    "   - It calls the existing `create_interaction_features` function (assuming it's defined) to handle interaction feature creation. This avoids modifying the original DataFrame unnecessarily.  \n",
    "- **Output:** It returns the modified DataFrame with any newly created custom features.\n",
    "\n",
    "Overall, this function empowers users to participate in feature engineering by defining new features based on their domain knowledge and understanding of the data. It offers flexibility in the approach and provides some guidance for expression-based feature creation. Remember to exercise caution with expression evaluation and potentially implement additional validation if needed for your specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **One-Hot Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_encoding(data, categorical_cols=None):\n",
    "  \"\"\"\n",
    "  Creates one-hot encoded features from categorical columns in a DataFrame.\n",
    "\n",
    "  Args:\n",
    "      data (pandas.DataFrame): The DataFrame containing the data.\n",
    "      categorical_cols (list, optional): A list of column names to encode. If None, all categorical columns will be considered. Defaults to None.\n",
    "\n",
    "  Returns:\n",
    "      pandas.DataFrame: The DataFrame with additional one-hot encoded features.\n",
    "  \"\"\"\n",
    "\n",
    "  if categorical_cols is None:\n",
    "    categorical_cols = [col for col in data.columns if data[col].dtype == \"object\"]\n",
    "\n",
    "  if not categorical_cols:\n",
    "    print(\"No categorical features found in the data. Skipping one-hot encoding.\")\n",
    "    return data\n",
    "\n",
    "  print(\"One-hot encoding is a technique for representing categorical features (like 'color' or 'size') as separate binary features.\")\n",
    "  print(\"Imagine a feature 'color' with values 'red', 'green', and 'blue'. One-hot encoding would create three new features:\")\n",
    "  print(\"  - 'color_red' (1 if the color is red, 0 otherwise)\")\n",
    "  print(\"  - 'color_green' (1 if the color is green, 0 otherwise)\")\n",
    "  print(\"  - 'color_blue' (1 if the color is blue, 0 otherwise)\")\n",
    "  print(\"This allows machine learning models to understand the relationships between these categories more effectively.\")\n",
    "  print(\"However, one-hot encoding can increase the number of features in your data significantly, which might require more computational resources.\")\n",
    "\n",
    "\n",
    "  # Get user confirmation to proceed\n",
    "  action = input(\"Do you want to create one-hot encoded features (y/n)? \").lower()\n",
    "  if action != \"y\":\n",
    "    print(\"Skipping one-hot encoding.\")\n",
    "    return data\n",
    "\n",
    "  # Informative message about one-hot encoding\n",
    "  print(\"One-hot encoding will create a separate binary feature for each unique category in a categorical column.\")\n",
    "\n",
    "  # Option to choose all or specific categorical features\n",
    "  while True:\n",
    "    choice = input(\"Choose encoding method (all/specific): \").lower()\n",
    "    if choice in [\"all\", \"specific\"]:\n",
    "      break\n",
    "    else:\n",
    "      print(\"Invalid choice. Please choose 'all' or 'specific'.\")\n",
    "\n",
    "  if choice == \"all\":\n",
    "    # Encode all categorical features\n",
    "    data = pd.get_dummies(data, columns=categorical_cols, drop_first=True)\n",
    "    print(\"Created one-hot encoded features for all categorical columns.\")\n",
    "\n",
    "  else:\n",
    "    # Prompt user to choose specific columns for encoding\n",
    "    selected_cols = []\n",
    "    while True:\n",
    "      col_name = input(\"Enter a categorical feature name (or 'done' to finish): \").lower()\n",
    "      if col_name == \"done\":\n",
    "        if not selected_cols:\n",
    "          print(\"No columns selected. Skipping one-hot encoding.\")\n",
    "        else:\n",
    "          data = pd.get_dummies(data, columns=selected_cols, drop_first=True)\n",
    "          print(f\"Created one-hot encoded features for selected columns.\")\n",
    "        break\n",
    "      elif col_name in categorical_cols:\n",
    "        selected_cols.append(col_name)\n",
    "        print(f\"Feature '{col_name}' added for one-hot encoding.\")\n",
    "      else:\n",
    "        print(f\"Invalid column name: '{col_name}'. Please choose from categorical features.\")\n",
    "\n",
    "  return data\n",
    "\n",
    "create_one_hot_encoding(data.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided function, `create_one_hot_encoding`, effectively addresses one-hot encoding for categorical features in a DataFrame. Here's a breakdown of its functionality:\n",
    "\n",
    "**Functionality:**\n",
    "\n",
    "1. **Input:**\n",
    "    - `data`: The DataFrame containing the data.\n",
    "    - `categorical_cols (optional)`: A list of column names to encode (defaults to all categorical columns).\n",
    "\n",
    "2. **Categorical Feature Identification:**\n",
    "    - If `categorical_cols` is not provided, it identifies all object data type columns as potential categorical features.\n",
    "    - It checks if any categorical features exist and informs the user if none are found.\n",
    "\n",
    "3. **Explanation of One-Hot Encoding:**\n",
    "    - If categorical features are present, it provides a clear explanation of one-hot encoding, including the creation of separate binary features for each unique category.\n",
    "    - It highlights both the benefit (improved model understanding) and the drawback (increased number of features) of one-hot encoding.\n",
    "\n",
    "4. **User Confirmation:**\n",
    "    - It asks the user for confirmation (\"y/n\") to proceed with creating one-hot encoded features.\n",
    "\n",
    "5. **Informative Message:**\n",
    "    - If the user confirms, it provides another informative message explaining how each categorical feature will have separate binary features.\n",
    "\n",
    "6. **Choice of Encoding Method:**\n",
    "    - It prompts the user to choose between encoding all categorical features or selecting specific ones (\"all/specific\").\n",
    "    - It uses a `while` loop to ensure a valid choice (\"all\" or \"specific\").\n",
    "\n",
    "7. **Encoding Process:**\n",
    "    - **All Categorical Features:** If \"all\" is chosen, it uses `pandas.get_dummies` to encode all categorical features specified in `categorical_cols`. It also sets `drop_first=True` to avoid creating dummy traps (redundant features).\n",
    "    - **Specific Categorical Features:** If \"specific\" is chosen, it uses a `while` loop to allow the user to enter column names one at a time. Valid selections are added to `selected_cols`. It uses `pandas.get_dummies` to encode only the chosen features with `drop_first=True`.\n",
    "\n",
    "8. **Output:**\n",
    "    - It returns the DataFrame (`data`) with additional one-hot encoded features.\n",
    "\n",
    "**Additional Notes:**\n",
    "\n",
    "- The function operates on a copy of the DataFrame (`data.copy()`) to avoid modifying the original data.\n",
    "- The informative messages and user interaction make the process user-friendly and transparent.\n",
    "\n",
    "Overall, this function provides a well-structured and informative approach to creating one-hot encoded features in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Label Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_encoding(data, categorical_cols=None):\n",
    "  \"\"\"\n",
    "  Creates label encoded features from categorical columns in a DataFrame.\n",
    "\n",
    "  Args:\n",
    "      data (pandas.DataFrame): The DataFrame containing the data.\n",
    "      categorical_cols (list, optional): A list of column names to encode. If None, all categorical columns will be considered. Defaults to None.\n",
    "\n",
    "  Returns:\n",
    "      pandas.DataFrame: The DataFrame with label encoded features (integers).\n",
    "  \"\"\"\n",
    "\n",
    "  if categorical_cols is None:\n",
    "    categorical_cols = [col for col in data.columns if data[col].dtype == \"object\"]\n",
    "\n",
    "  if not categorical_cols:\n",
    "    print(\"No categorical features found in the data. Skipping label encoding.\")\n",
    "    return data\n",
    "\n",
    "  print(\"Label encoding is a simpler way to handle categorical features. It assigns a unique number to each different category.\")\n",
    "  print(\"For example, a feature 'fruit' with values 'apple', 'banana', and 'orange' might be encoded as:\")\n",
    "  print(\"  - apple: 0\")\n",
    "  print(\"  - banana: 1\")\n",
    "  print(\"  - orange: 2\")\n",
    "  print(\"This allows machine learning models to process the data more easily. However, it's important to be aware of a potential drawback:\")\n",
    "  print(\"  - Label encoding might treat higher numbers as more 'important' even if the categories have no inherent order.\")\n",
    "  print(\"For example, 'orange' (encoded as 2) might seem 'better' than 'apple' (encoded as 0) to the model, even though they are just different fruits.\")\n",
    "  print(\"If the order of your categories doesn't matter, label encoding can be a good choice. But if the order is important, you might want to consider other encoding techniques.\")\n",
    "\n",
    "\n",
    "  # Get user confirmation to proceed\n",
    "  action = input(\"Do you want to create label encoded features (y/n)? \").lower()\n",
    "  if action != \"y\":\n",
    "    print(\"Skipping label encoding.\")\n",
    "    return data\n",
    "\n",
    "  # Informative message about label encoding\n",
    "  print(\"Label encoding assigns a unique integer value to each category in a categorical column.\")\n",
    "  print(\"** Caution:** This might introduce unintended ordering between categories.\")\n",
    "\n",
    "  # Option to choose all or specific categorical features\n",
    "  while True:\n",
    "    choice = input(\"Choose encoding method (all/specific): \").lower()\n",
    "    if choice in [\"all\", \"specific\"]:\n",
    "      break\n",
    "    else:\n",
    "      print(\"Invalid choice. Please choose 'all' or 'specific'.\")\n",
    "\n",
    "  if choice == \"all\":\n",
    "    # Encode all categorical features\n",
    "    for col in categorical_cols:\n",
    "      le = sklearn.preprocessing.LabelEncoder()\n",
    "      data[col] = le.fit_transform(data[col])\n",
    "    print(\"Created label encoded features for all categorical columns.\")\n",
    "\n",
    "  else:\n",
    "    # Prompt user to choose specific columns for encoding\n",
    "    selected_cols = []\n",
    "    while True:\n",
    "      col_name = input(\"Enter a categorical feature name (or 'done' to finish): \").lower()\n",
    "      if col_name == \"done\":\n",
    "        if not selected_cols:\n",
    "          print(\"No columns selected. Skipping label encoding.\")\n",
    "        else:\n",
    "          for col in selected_cols:\n",
    "            le = sklearn.preprocessing.LabelEncoder()\n",
    "            data[col] = le.fit_transform(data[col])\n",
    "          print(f\"Created label encoded features for selected columns.\")\n",
    "        break\n",
    "      elif col_name in categorical_cols:\n",
    "        selected_cols.append(col_name)\n",
    "        print(f\"Feature '{col_name}' added for label encoding.\")\n",
    "      else:\n",
    "        print(f\"Invalid column name: '{col_name}'. Please choose from categorical features.\")\n",
    "\n",
    "  return data\n",
    "\n",
    "create_label_encoding(data.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely, here's a summary of the `create_label_encoding` function:\n",
    "\n",
    "**Functionality:**\n",
    "\n",
    "1. **Input:**\n",
    "    - `data`: The DataFrame containing the data.\n",
    "    - `categorical_cols (optional)`: A list of column names to encode (defaults to all categorical columns).\n",
    "\n",
    "2. **Categorical Feature Identification:**\n",
    "    - If `categorical_cols` is not provided, it identifies all object data type columns as potential categorical features.\n",
    "    - It checks if any categorical features exist and informs the user if none are found.\n",
    "\n",
    "3. **Explanation of Label Encoding:**\n",
    "    - If categorical features are present, it provides a clear explanation of label encoding, including assigning unique integer values to each category.\n",
    "    - It showcases an example for clarity.\n",
    "    - It highlights the benefit (easier processing for models) and the drawback (potential introduction of unintended order) of label encoding.\n",
    "    - It emphasizes the importance of considering the inherent order of categories when choosing label encoding.\n",
    "\n",
    "4. **User Confirmation:**\n",
    "    - It asks the user for confirmation (\"y/n\") to proceed with creating label encoded features.\n",
    "\n",
    "5. **Informative Message:**\n",
    "    - If the user confirms, it provides another informative message explaining the integer assignment to categories and a cautionary note about potential order assumptions.\n",
    "\n",
    "6. **Choice of Encoding Method:**\n",
    "    - It prompts the user to choose between encoding all categorical features or selecting specific ones (\"all/specific\").\n",
    "    - It uses a `while` loop to ensure a valid choice (\"all\" or \"specific\").\n",
    "\n",
    "7. **Encoding Process:**\n",
    "    - **All Categorical Features:** If \"all\" is chosen, it iterates through each column in `categorical_cols`. Inside the loop, it creates a `LabelEncoder` object from `sklearn.preprocessing`. It uses `fit_transform` on the corresponding column in the DataFrame to encode the categories and update the column with the encoded values.\n",
    "    - **Specific Categorical Features:** If \"specific\" is chosen, it uses a `while` loop to allow the user to enter column names one at a time. Valid selections are added to `selected_cols`. It follows a similar approach as \"all\" but iterates only through `selected_cols` to encode specific features.\n",
    "\n",
    "8. **Output:**\n",
    "    - It returns the DataFrame (`data`) with the categorical columns replaced by their label encoded integer values.\n",
    "\n",
    "**Additional Notes:**\n",
    "\n",
    "- The function operates on a copy of the DataFrame (`data.copy()`) to avoid modifying the original data.\n",
    "- The informative messages and user interaction make the process user-friendly and transparent.\n",
    "\n",
    "Overall, this function provides a well-structured and informative approach to creating label encoded features in Python. It effectively explains the concepts and guides the user through the process while highlighting potential limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Handling Class Imbalance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_class_imbalance(data, target_col):\n",
    "  \"\"\"\n",
    "  Provides options to handle class imbalance in a dataset.\n",
    "\n",
    "  Args:\n",
    "      data (pandas.DataFrame): The DataFrame containing the data.\n",
    "      target_col (str): The name of the column containing the target variable.\n",
    "\n",
    "  Returns:\n",
    "      pandas.DataFrame: The DataFrame with potentially balanced classes.\n",
    "  \"\"\"\n",
    "\n",
    "  # Display class distribution\n",
    "  print(\"** Class Distribution:\")\n",
    "  class_counts = data[target_col].value_counts().sort_values(ascending=False)\n",
    "  print(class_counts)\n",
    "\n",
    "  # Check for imbalance\n",
    "  majority_class = class_counts.index[0]\n",
    "  majority_count = class_counts.iloc[0]\n",
    "  imbalanced = majority_count / len(data) > 0.5  # Ratio check for imbalance\n",
    "\n",
    "  if not imbalanced:\n",
    "    print(\"Class distribution seems balanced. Skipping imbalance handling.\")\n",
    "    return data\n",
    "\n",
    "  # Explain class imbalance\n",
    "  print(\"\\n** What is Class Imbalance?**\")\n",
    "  print(\"In machine learning, class imbalance occurs when a classification task has a significant skew\")\n",
    "  print(\"in the number of examples between different classes. Typically, one class (the majority class)\")\n",
    "  print(\"has many more examples than the other classes (the minority class).\")\n",
    "  print(\"This imbalance can lead to models that are biased towards the majority class and perform poorly\")\n",
    "  print(\"on the minority class.\")\n",
    "\n",
    "  # Get user choice for handling imbalance\n",
    "  print(\"** Handling Class Imbalance:\")\n",
    "  print(\"- Undersampling (reduce majority class size)\")\n",
    "  print(\"  - Recommended if the majority class might be noisy or irrelevant.\")\n",
    "  print(\"- Oversampling (increase minority class size)\")\n",
    "  print(\"  - Recommended if the minority class is informative and you have enough data.\")\n",
    "  print(\"  - We will use the Synthetic Minority Oversampling Technique (SMOTE) for oversampling to avoid overfitting.\")\n",
    "  print(\"- No action (continue with imbalanced data)\")\n",
    "  print(\"  - Only recommended if the class imbalance doesn't significantly affect the model.\")\n",
    "\n",
    "\n",
    "  while True:\n",
    "    choice = input(\"Choose an option (undersample/oversample/none): \").lower()\n",
    "    if choice in [\"undersample\", \"oversample\", \"none\"]:\n",
    "      break\n",
    "    else:\n",
    "      print(\"Invalid choice. Please choose 'undersample', 'oversample', or 'none'.\")\n",
    "\n",
    "  if choice == \"none\":\n",
    "    print(\"Continuing with imbalanced data.\")\n",
    "    return data\n",
    "\n",
    "  # Handle undersampling or oversampling based on user choice\n",
    "  if choice in [\"undersample\", \"oversample\"]:\n",
    "    print(f\"Selected '{choice}'.\")\n",
    "    sampling_ratio = float(input(\"Enter desired sampling ratio (between 0 and 1): \"))\n",
    "    if sampling_ratio <= 0 or sampling_ratio > 1:\n",
    "      print(\"Invalid sampling ratio. Please enter a value between 0 and 1.\")\n",
    "      return data  # Avoid errors with invalid ratio\n",
    "\n",
    "    from imblearn.under_sampling import RandomUnderSampler  # Import for undersampling\n",
    "    from imblearn.over_sampling import SMOTE  # Import for oversampling\n",
    "\n",
    "    if choice == \"undersample\":\n",
    "      rus = RandomUnderSampler(sampling_strategy={majority_class: int(sampling_ratio * majority_count)})\n",
    "      data = rus.fit_resample(data, data[target_col])\n",
    "      print(f\"Undersampled majority class to {int(sampling_ratio * majority_count)} samples.\")\n",
    "    else:\n",
    "      sm = SMOTE(sampling_strategy={target_col: \"auto\"})\n",
    "      data = sm.fit_resample(data, data[target_col])\n",
    "      print(f\"Oversampled minority class to match the majority class size.\")\n",
    "\n",
    "  # Display final class distribution\n",
    "  print(\"** Final Class Distribution:\")\n",
    "  class_counts = data[target_col].value_counts().sort_values(ascending=False)\n",
    "  print(class_counts)\n",
    "\n",
    "  return data\n",
    "\n",
    "handle_class_imbalance(data.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely, here's a summary of the `handle_class_imbalance` function:\n",
    "\n",
    "**Functionality:**\n",
    "\n",
    "1. **Input:**\n",
    "    - `data`: The DataFrame containing the data.\n",
    "    - `target_col`: The name of the column containing the target variable (class labels).\n",
    "\n",
    "2. **Initial Analysis:**\n",
    "    - It displays the class distribution using `value_counts` to show the number of examples for each class.\n",
    "    - It calculates the ratio of the majority class size to the total number of samples to identify imbalance (ratio > 0.5 signifies imbalance).\n",
    "\n",
    "3. **Class Imbalance Explanation (if applicable):**\n",
    "    - If the data is imbalanced, it provides a clear explanation of class imbalance, its consequences (biased models towards the majority class), and its impact on model performance.\n",
    "\n",
    "4. **User Choice for Handling Imbalance (if applicable):**\n",
    "    - If the data is imbalanced, it presents the user with options to handle the imbalance:\n",
    "        - Undersampling (reduce majority class size) - recommended for noisy or irrelevant majority class.\n",
    "        - Oversampling (increase minority class size) - recommended for informative minority class with sufficient data. It mentions using SMOTE for oversampling to avoid overfitting.\n",
    "        - No action (continue with imbalanced data) - only recommended if the imbalance has minimal impact on the model.\n",
    "    - It uses a `while` loop to ensure a valid choice (\"undersample\", \"oversample\", or \"none\").\n",
    "\n",
    "5. **Handling Imbalance (if applicable):**\n",
    "    - Based on the user's choice:\n",
    "        - **No Action:** If \"none\" is chosen, it informs the user and returns the original data.\n",
    "        - **Undersampling or Oversampling:** \n",
    "            - It prompts the user for a desired sampling ratio (between 0 and 1).\n",
    "            - It imports necessary libraries (`imblearn.under_sampling` or `imblearn.over_sampling`).\n",
    "            - If undersampling is chosen, it creates a `RandomUnderSampler` object with the desired sampling ratio for the majority class and uses `fit_resample` to reduce its size.\n",
    "            - If oversampling is chosen, it creates an `SMOTE` object with the target column (`target_col`) and uses `fit_resample` to increase the minority class size to match the majority.\n",
    "            - It provides feedback on the sampling action performed.\n",
    "\n",
    "6. **Final Class Distribution:**\n",
    "    - It displays the final class distribution after any potential balancing actions.\n",
    "\n",
    "7. **Output:**\n",
    "    - It returns the DataFrame (`data`) with potentially balanced classes (depending on the user's choice).\n",
    "\n",
    "**Additional Notes:**\n",
    "\n",
    "- The function operates on a copy of the DataFrame (`data.copy()`) to avoid modifying the original data.\n",
    "- User interaction allows for informed decision-making about handling class imbalance.\n",
    "- Informative messages guide the user through the process.\n",
    "\n",
    "Overall, this function provides a well-structured and informative approach to addressing class imbalance in Python. It effectively balances user guidance with clear explanations and execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = input(\"Enter the name of the column containing the target variable (the variable you wish to predict/classify):\")\n",
    "\n",
    "def feature_selection(data, target_column):\n",
    "  \"\"\"\n",
    "  Provides options for feature selection in machine learning tasks.\n",
    "\n",
    "  Args:\n",
    "      data (pandas.DataFrame): The DataFrame containing the data.\n",
    "      target_col (str): The name of the column containing the target variable.\n",
    "\n",
    "  Returns:\n",
    "      pandas.DataFrame: The DataFrame with potentially reduced features.\n",
    "  \"\"\"\n",
    "\n",
    "  # Display initial information\n",
    "  print(\"** Feature Selection helps identify the most relevant features for your machine learning model.\")\n",
    "  print(\"It can improve model performance, reduce training time, and make the model easier to interpret.\")\n",
    "\n",
    "  # Get user preference for selection method\n",
    "  print(\"\\n** Feature Selection Methods:\")\n",
    "  print(\"- Filter Methods (based on statistical tests for individual features)\")\n",
    "  print(\"- Wrapper Methods (use a machine learning model to evaluate feature subsets)\")\n",
    "  print(\"- Embedded Methods (integrated within a machine learning model)\")\n",
    "  print(\"\\n** We will focus on Filter Methods for this session.**\")\n",
    "\n",
    "  while True:\n",
    "    choice = input(\"Do you want to proceed with Filter Methods (y/n)? \").lower()\n",
    "    if choice in [\"y\", \"n\"]:\n",
    "      break\n",
    "    else:\n",
    "      print(\"Invalid choice. Please choose 'y' or 'n'.\")\n",
    "\n",
    "  if choice == \"n\":\n",
    "    print(\"Skipping feature selection. Using all features.\")\n",
    "    return data\n",
    "\n",
    "  # Filter Method Selection\n",
    "  print(\"\\n** Filter Methods Options:\")\n",
    "  print(\"- Select K Best (choose a specific number of features)\")\n",
    "  print(\"- Select Percentile (choose a percentage of features)\")\n",
    "  print(\"** We will use Select K Best for this session.**\")\n",
    "\n",
    "  # Select K Best configuration\n",
    "  while True:\n",
    "    try:\n",
    "      k = int(input(\"Enter the desired number of features to select (integer): \"))\n",
    "      if k > 0:\n",
    "        break\n",
    "      else:\n",
    "        print(\"Invalid number. Please enter a positive integer.\")\n",
    "    except ValueError:\n",
    "      print(\"Invalid input. Please enter an integer.\")\n",
    "\n",
    "  # Feature selection using SelectKBest\n",
    "  from sklearn.feature_selection import SelectKBest\n",
    "  from sklearn.feature_selection import chi2  # Example statistical test\n",
    "\n",
    "  X = data.drop(target_col, axis=1)  # Separate features (X) and target (y)\n",
    "  y = data[target_col]\n",
    "\n",
    "  selector = SelectKBest(chi2, k=k)  # Use chi-square test for filter\n",
    "  selector.fit(X, y)\n",
    "  selected_features = X.columns[selector.get_support(indices=True)]\n",
    "\n",
    "  # Informative output\n",
    "  print(f\"\\n** Selected Features using SelectKBest (chi-square):**\")\n",
    "  for feature in selected_features:\n",
    "    print(f\"- {feature}\")\n",
    "\n",
    "  # User confirmation for using selected features\n",
    "  print(\"\\n** These features will be used for model training.**\")\n",
    "  while True:\n",
    "    choice = input(\"Continue with selected features (y/n)? \").lower()\n",
    "    if choice in [\"y\", \"n\"]:\n",
    "      break\n",
    "    else:\n",
    "      print(\"Invalid choice. Please choose 'y' or 'n'.\")\n",
    "\n",
    "  if choice == \"n\":\n",
    "    print(\"Original features will be used for model training.\")\n",
    "    return data\n",
    "\n",
    "  # Return DataFrame with selected features\n",
    "  return data[selected_features]\n",
    "\n",
    "feature_selection(data.copy(), target_column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python function, `feature_selection`, offers an interactive interface to assist with feature selection in machine learning tasks. It guides the user through the process, focusing on Filter Methods (specifically, Select K Best in this version). Here's a breakdown:\n",
    "\n",
    "- **Input:** It takes a DataFrame (`data`) containing your features and target variable.\n",
    "- **Functionality:**\n",
    "    - Explains the benefits of feature selection.\n",
    "    - Prompts the user to choose between Filter Methods (current focus) or skipping selection altogether.\n",
    "    - If Filter Methods are chosen, it provides basic explanations of options (Select K Best, Select Percentile).\n",
    "    - It guides the user through selecting a desired number of features (k) for Select K Best.\n",
    "    - It performs feature selection using `SelectKBest` with chi-square test (as an example) to identify relevant features.\n",
    "    - It informs the user about the selected features.\n",
    "    - It asks for confirmation before using the selected features and modifying the DataFrame.\n",
    "- **Output:** It returns a new DataFrame potentially containing a reduced set of features based on user choices and the selected method.\n",
    "\n",
    "This function prioritizes user interaction and offers a starting point for feature selection. It's important to note that running it on every column might not be ideal, and other techniques like correlation analysis or feature importance scores can be valuable tools alongside it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
