{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  # for nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Type Conversion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Types\n",
    "data_types = data.dtypes\n",
    "print(\"Data Types for Each Column in Your Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def convert_data_types(data):\n",
    "  \"\"\"\n",
    "  Prints data types for each column and allows user to change them.\n",
    "\n",
    "  Args:\n",
    "      data (pandas.DataFrame): The DataFrame containing the data.\n",
    "\n",
    "  Returns:\n",
    "      pandas.DataFrame: The DataFrame potentially with changed data types.\n",
    "  \"\"\"\n",
    "\n",
    "  # Explain data types in a dictionary for easy reference\n",
    "  dtype_explanations = {\n",
    "      'int64': \"Integer (whole numbers, positive or negative)\",\n",
    "      'float64': \"Decimal number\",\n",
    "      'object': \"Text data (strings)\",\n",
    "      'category': \"Categorical data (limited set of options)\",\n",
    "      'datetime64[ns]': \"Date and time\",\n",
    "      'bool': \"Boolean (True or False)\"\n",
    "  }\n",
    "\n",
    "  # Print data types with explanations\n",
    "  for col, dtype in data_types.items():\n",
    "    print(f\"- {col}: {dtype} ({dtype_explanations.get(dtype, 'Unknown')})\")\n",
    "\n",
    "  # Prompt user for data type changes\n",
    "  change_dtypes = input(\"Would you like to change any data types (y/n)? \").lower()\n",
    "  if change_dtypes == \"y\":\n",
    "    while True:\n",
    "      # Ask for column and desired data type\n",
    "      col_to_change = input(\"Enter the column name to change the data type: \").lower()\n",
    "      new_dtype = input(\"Enter the desired new data type (int, float, object, etc.): \").lower()\n",
    "\n",
    "      # Check if column exists and new data type is valid\n",
    "      if col_to_change in data.columns and new_dtype in dtype_explanations.keys():\n",
    "        try:\n",
    "          # Attempt conversion (handles potential errors)\n",
    "          data[col_to_change] = data[col_to_change].astype(new_dtype)\n",
    "          print(f\"Data type for '{col_to_change}' changed to {new_dtype}.\")\n",
    "          # **Modified break logic:**\n",
    "          break_loop = input(\"Do you want to convert another column (y/n)? \").lower()\n",
    "          if break_loop != \"y\":\n",
    "            break\n",
    "        except (ValueError, TypeError) as e:\n",
    "          print(f\"Error converting '{col_to_change}' to {new_dtype}: {e}\")\n",
    "          # **Prompt to continue after error**\n",
    "          continue_loop = input(\"Would you like to try converting another column (y/n)? \").lower()\n",
    "          if continue_loop != \"y\":\n",
    "            break\n",
    "      else:\n",
    "        print(f\"Invalid column name or data type. Please try again.\")\n",
    "\n",
    "  return data\n",
    "\n",
    "# Example usage \n",
    "data = convert_data_types(data.copy())  # Avoid modifying original data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focus: Data cleaning addresses inconsistencies, errors, and missing values within the data itself.\n",
    "\n",
    "Data Type Conversion: In this context, converting data types is often a cleaning step when the data type is incorrect or incompatible with how the data should be represented.\n",
    "Examples:\n",
    "- Inconsistent date formats (text to datetime).\n",
    "- Text values in numerical columns (text to numerical).\n",
    "- Incorrect data types due to import issues (e.g., strings instead of integers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dealing With Normality and Skewness**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most efficient way to assess normality and skewness in your data columns depends on a few factors:\n",
    "\n",
    "**1. Number of Columns:**\n",
    "\n",
    "* **Few Columns:** `(still need implementation)` If you have a small number of columns (less than 10), visual inspection using histograms and QQ plots might be the most efficient approach. These techniques are easy to understand and interpret, providing a quick grasp of the data distribution.\n",
    "\n",
    "* **Many Columns:** With a large number of columns (more than 10), visual inspection becomes cumbersome. Here, statistical tests like the Shapiro-Wilk test can be more efficient. You can calculate the test statistic and p-value for each column to identify potential deviations from normality. A threshold for the p-value (e.g., 0.05) can be used to decide if the data is likely non-normal.\n",
    "\n",
    "**2. Desired Level of Detail:**\n",
    "\n",
    "* **Basic Assessment:** If you just need a quick indication of normality or skewness, histograms and statistical tests with p-values provide a sufficient level of detail.\n",
    "\n",
    "* **Detailed Analysis:** For a more in-depth analysis, you can combine both approaches. Start with histograms and QQ plots to get a visual sense of the distribution, and then follow up with statistical tests to confirm your observations or explore borderline cases with p-values close to the chosen threshold.\n",
    "\n",
    "Here's a breakdown of the efficiency considerations:\n",
    "\n",
    "| Method | Efficiency for Few Columns | Efficiency for Many Columns | Level of Detail |\n",
    "|---|---|---|---|\n",
    "| Histograms & QQ Plots | High (easy to interpret visually) | Low (time-consuming for many columns) | High (visual assessment of shape) |\n",
    "| Statistical Tests | Low (requires calculations) | High (efficient for many columns) | Moderate (p-value indicates normality likelihood) |\n",
    "\n",
    "**Combined Approach:**\n",
    "\n",
    "In practice, a combination of visual inspection and statistical tests often offers the best balance between efficiency and detail. Start with histograms and QQ plots for a quick overview, then use statistical tests for more rigorous confirmation, especially when dealing with many columns.\n",
    "\n",
    "Here are some additional factors to consider:\n",
    "\n",
    "* **Computational Resources:** If computational resources are limited, visual methods might be preferred. Statistical tests, especially for large datasets, can require more processing power.\n",
    "* **Domain Knowledge:** If you have domain knowledge about the data, you might have an initial expectation about the normality of certain features. This can guide your choice of method, focusing on tests for features where normality is critical for your analysis.\n",
    "\n",
    "Ultimately, the most efficient approach depends on your specific needs and the size of your dataset. Combining visual and statistical methods often provides a comprehensive and efficient way to assess normality and skewness in your data columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Normalizing/Scaling Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "def scale(data):\n",
    "  \"\"\"\n",
    "  Identifies skewed features, suggests corrections, and performs scaling/normalization.\n",
    "\n",
    "  Args:\n",
    "      data (pandas.DataFrame): The DataFrame containing the data.\n",
    "\n",
    "  Returns:\n",
    "      pandas.DataFrame: The transformed DataFrame with addressed skewness and scaling/normalization.\n",
    "  \"\"\"\n",
    "  numerical_cols = data.select_dtypes(include=[np.number])\n",
    "  skewed_cols = []  # List to store column names with skewness\n",
    "\n",
    "  # Threshold for skewness (adjust as needed)\n",
    "  skewness_threshold = 0.5\n",
    "\n",
    "  for col in numerical_cols:\n",
    "    # Calculate skewness\n",
    "    skew = data[col].skew()\n",
    "    if abs(skew) > skewness_threshold:\n",
    "      skewed_cols.append(col)\n",
    "      print(f\"Column '{col}' appears skewed (skewness: {skew:.2f}).\")\n",
    "\n",
    "\n",
    "      # Inform decision-making\n",
    "      print(\"Here's a brief explanation of the available correction methods:\")\n",
    "      print(\"  - Log transformation (log(x + 1)): This method is often effective for right-skewed data (where values are concentrated on the left side of the distribution).\")\n",
    "      print(\"    It compresses the larger values and stretches the smaller ones, aiming for a more symmetrical distribution.\")\n",
    "      print(\"  - Square root transformation (sqrt(x)): This method can be helpful for moderately skewed data, positive-valued features, or data with a large number of zeros.\")\n",
    "      print(\"    It reduces the influence of extreme values and can bring the distribution closer to normality.\")\n",
    "      print(\"**Please consider the characteristics of your skewed feature(s) when making your choice.**\")\n",
    "      print(\"If you're unsure, you can experiment with both methods and compare the results visually (e.g., using histograms) to see which one normalizes the data more effectively for your specific case.\")\n",
    "\n",
    "      # User prompt for addressing skewness\n",
    "      action = input(\"Do you want to address the skewness (y/n)? \").lower()\n",
    "      if action == \"y\":\n",
    "        \n",
    "\n",
    "        # User chooses to address skewness\n",
    "        while True:  # Loop until a valid choice is made\n",
    "          fix_method = input(\"Choose a correction method (log/sqrt/none): \").lower()\n",
    "          if fix_method in [\"log\", \"sqrt\"]:\n",
    "            # Apply transformation (log or sqrt)\n",
    "            if fix_method == \"log\":\n",
    "              data[col] = np.log(data[col] + 1)  # Avoid log(0) errors by adding 1\n",
    "              print(f\"Applied log transformation to column '{col}'.\")\n",
    "            else:\n",
    "              data[col] = np.sqrt(data[col])\n",
    "              print(f\"Applied square root transformation to column '{col}'.\")\n",
    "            break  # Exit the loop if a valid choice is made\n",
    "          else:\n",
    "            print(\"Invalid choice. Please choose 'log', 'sqrt', or 'none'.\")\n",
    "\n",
    "      else:\n",
    "        print(f\"Skewness in '{col}' remains unaddressed.\")\n",
    "    \n",
    "    if not skewed_cols:\n",
    "      print(\"No significant skewness detected in numerical columns.\")\n",
    "\n",
    "  # User prompt for scaling/normalization (if applicable)\n",
    "  if len(numerical_cols) > 0:\n",
    "\n",
    "    print(\"Here's a brief explanation of the available scaling/normalization methods:\")\n",
    "    print(\"  - Standard scaling: This method transforms features by subtracting the mean and dividing by the standard deviation.\")\n",
    "    print(\"    This results in features centered around zero with a standard deviation of 1.\")\n",
    "    print(\"    It's suitable for algorithms that assume a normal distribution of features (e.g., Logistic Regression, Support Vector Machines).\")\n",
    "    print(\"  - Min-max scaling: This method scales each feature to a specific range, typically between 0 and 1.\")\n",
    "    print(\"    It achieves this by subtracting the minimum value and then dividing by the difference between the maximum and minimum values in the feature.\")\n",
    "    print(\"    This can be useful for algorithms that are sensitive to the scale of features (e.g., K-Nearest Neighbors).\")\n",
    "    print(\"**Choosing the right method depends on your data and the algorithm you're using.**\")\n",
    "    print(\"  - If you're unsure about the underlying distribution of your data, standard scaling might be a safer choice as it doesn't make assumptions about normality.\")\n",
    "    print(\"  - If your algorithm is sensitive to feature scales and doesn't assume normality, min-max scaling might be preferable.\")\n",
    "    print(\"Consider the characteristics of your data and algorithm when making your decision. You can also experiment with both methods\")\n",
    "    print(\"and compare the results using model performance metrics to see which one works best for your specific case.\")\n",
    "\n",
    "    action = input(\"Do you want to scale or normalize the numerical features (y/n)? \").lower()\n",
    "    if action == \"y\":\n",
    "\n",
    "      while True:  # Loop until a valid choice is made\n",
    "        method = input(\"Choose scaling/normalization method (standard/minmax/skip): \").lower()\n",
    "        if method in [\"standard\", \"minmax\"]:\n",
    "          if method == \"standard\":\n",
    "            scaler = StandardScaler()\n",
    "            data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "            print(f\"Applied standard scaling to numerical features.\")\n",
    "          else:\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "            print(f\"Applied min-max scaling to numerical features (range 0-1).\")\n",
    "          break  # Exit the loop if a valid choice is made\n",
    "        elif method == \"skip\":\n",
    "          print(\"Skipping scaling/normalization.\")\n",
    "          break\n",
    "        else:\n",
    "          print(\"Invalid choice. Please choose 'standard', 'minmax', or 'skip'.\")\n",
    "\n",
    "\n",
    "  if not skewed_cols:\n",
    "    print(\"No significant skewness detected in numerical columns.\")\n",
    "  return data\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "preprocessed_data = scale(data.copy())  # Operate on a copy to avoid modifying original data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scale` function offers a user-guided approach to data preprocessing, addressing both skewness in numerical features and optional scaling/normalization. Here's a breakdown of its functionalities:\n",
    "\n",
    "1. **Skewness Identification:**\n",
    "   - It identifies features with significant skewness (asymmetry in the distribution) based on a user-defined threshold.\n",
    "   - For skewed features, it informs the user about the skewness value and prompts them to address it.\n",
    "\n",
    "2. **Skewness Correction (Optional):**\n",
    "   - If the user chooses to address skewness, it offers options for log or square root transformation to potentially reduce skewness.\n",
    "   - The chosen transformation is applied to the specific feature(s).\n",
    "\n",
    "3. **Scaling/Normalization (Optional):**\n",
    "   - After addressing skewness (or if no skewness is found), it prompts the user to decide if they want to scale or normalize the numerical features.\n",
    "   - If the user chooses to proceed, it offers options for standard scaling (centering and scaling features to have zero mean and unit variance) or min-max scaling (scaling features to a specific range, typically 0-1).\n",
    "   - The chosen scaling/normalization method is applied using scikit-learn's `StandardScaler` or `MinMaxScaler` to transform the numerical features.\n",
    "\n",
    "\n",
    "Overall, this function simplifies data preprocessing by guiding the user through common steps like handling skewness and applying scaling/normalization techniques. It provides flexibility and explanations, making it easier to understand and customize the preprocessing process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Label Encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Handling Class Imbalance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Selection**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
