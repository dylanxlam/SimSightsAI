{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **data_handler/preprocessing.py**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Reading Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_data\n",
    "\n",
    "file = input(\"Please upload your data file. We support CSV, Excel, TSV, and JSON\")\n",
    "\n",
    "def read_data(file):\n",
    "    \"\"\"\n",
    "    Reads data from uploaded file (supports CSV, Excel, TSV, JSON).\n",
    "\n",
    "    Args:\n",
    "        file (object): The uploaded file object from Flask request.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame (or list/dict): The loaded data in a suitable format.\n",
    "    \"\"\"\n",
    "    # Identify file format based on filename extension or MIME type (consider using magic library)\n",
    "    if file.filename.endswith(\".csv\"):\n",
    "        data = pd.read_csv(file)\n",
    "    elif file.filename.endswith(\".xlsx\"):\n",
    "        data = pd.read_excel(file)\n",
    "    elif file.filename.endswith(\".tsv\"):\n",
    "        data = pd.read_csv(file, sep=\"\\t\")  # Use tab separator for TSV\n",
    "    elif file.filename.endswith(\".json\"):\n",
    "        try:\n",
    "            data = json.load(file)  # Assuming JSON data represents a list or dictionary\n",
    "        except json.JSONDecodeError:\n",
    "            raise ValueError(\"Invalid JSON format. Please check your data.\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please upload CSV, Excel, TSV, or JSON files.\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Can add logic for handling image formats (consider for future development)\n",
    "    # elif file.content_type.startswith(\"image/\"):  # Check for image content type\n",
    "    #     # Implement image loading and pre-processing logic here (libraries like OpenCV)\n",
    "    #     # Return a suitable data structure for image data\n",
    "    #     pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Descriptive Statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Descriptive Statistics\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data_descriptive_statistics \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mdescribe()\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDescriptive Statistics of Your Data:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, data_descriptive_statistics)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Descriptive Statistics\n",
    "data_descriptive_statistics = data.describe()\n",
    "print(\"Descriptive Statistics of Your Data:\\n\", data_descriptive_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will display summary statistics for numerical columns in your data, including:\n",
    "\n",
    "- count: The number of non-null values in each column.\n",
    "- mean: The average value.\n",
    "- std: The standard deviation.\n",
    "- min: The minimum value.\n",
    "- 25%: The first quartile (25th percentile).\n",
    "- 50%: The median (50th percentile).\n",
    "- 75%: The third quartile (75th percentile).\n",
    "- max: The maximum value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how the summary statistics from `data.describe()` can inform data cleaning methods for robust modeling:\n",
    "\n",
    "**1. Analyzing Central Tendency:**\n",
    "\n",
    "* **Mean & Median:** These statistics represent the \"average\" value in a column. Significant deviations between mean and median can indicate a skewed distribution.\n",
    "\n",
    "* **Skew:** This statistic directly measures the skewness of the data. A positive skew indicates more data points concentrated towards lower values, while negative skew suggests a tail towards higher values.\n",
    "\n",
    "**Implications for Cleaning:**\n",
    "\n",
    "* Skewed data can affect the performance of some machine learning models. Depending on the model and the severity of the skew, you might consider data transformation (e.g., log transformation) or using models robust to skewed data.\n",
    "\n",
    "**2. Understanding Dispersion:**\n",
    "\n",
    "* **Standard Deviation (Std):** This statistic shows how spread out the data is from the mean. A high Std indicates high variability, while a low Std suggests the data is clustered around the mean.\n",
    "\n",
    "* **Minimum & Maximum:** These values reveal the range of the data. Outliers (values far from the rest) can be identified by comparing them to the IQR (Interquartile Range) or a certain number of standard deviations from the mean.\n",
    "\n",
    "**Implications for Cleaning:**\n",
    "\n",
    "* Outliers can significantly impact some models. You might need to decide on handling outliers through winsorization (capping them to a certain threshold), removal, or using models less sensitive to outliers.\n",
    "\n",
    "**3. Exploring Data Types:**\n",
    "\n",
    "* **Data Types:** `data.describe()` often shows the data type (e.g., int, float) of each column. Inconsistencies or incorrect data types (e.g., dates stored as text) can lead to errors in modeling.\n",
    "\n",
    "**Implications for Cleaning:**\n",
    "\n",
    "* You might need to convert data types (e.g., text to numeric for numerical features, handling dates appropriately) to ensure compatibility with modeling algorithms.\n",
    "\n",
    "**Overall, `data.describe()` provides a high-level overview of the data's central tendency, dispersion, and potential issues like missing values and outliers. By analyzing these statistics, you can identify areas where data cleaning is necessary to prepare your data for robust modeling.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Handling Null Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Null Values for each column\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m missing_values \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39misnull()\u001b[39m.\u001b[39msum()\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMissing/Null Values for Each Column/Feature of Your Data:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, missing_values)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Null Values for each column\n",
    "missing_values = data.isnull().sum()\n",
    "print(\"Missing/Null Values for Each Column/Feature of Your Data:\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  # for nan\n",
    "\n",
    "def handle_missing_values(data):\n",
    "  \"\"\"\n",
    "  Analyzes and guides the user on handling missing values in a DataFrame.\n",
    "\n",
    "  Args:\n",
    "      data (pandas.DataFrame): The DataFrame containing the data.\n",
    "\n",
    "  Returns:\n",
    "      pandas.DataFrame: The DataFrame potentially with imputed missing values\n",
    "                          based on the user's choice.\n",
    "  \"\"\"\n",
    "  # Check for missing values\n",
    "\n",
    "  # No missing values\n",
    "  \n",
    "  if missing_values.sum() == 0:\n",
    "    print(\"Good, no null values to deal with in this dataset!\")\n",
    "  else:\n",
    "    # Inform user about missing values and their importance\n",
    "    print(\"There are missing values in your data! It's crucial to address them\")\n",
    "    print(\"before further analysis. Missing values can skew results and lead to\")\n",
    "    print(\"inaccurate conclusions. Let's handle them!\")\n",
    "\n",
    "    # Prompt user for null value technique (loop for repeated input validation)\n",
    "    while True:\n",
    "      method_choice = input(\"Choose a method to handle missing values (deletion, imputation, encoding): \").lower()\n",
    "\n",
    "      # User chooses deletion\n",
    "      if method_choice == \"deletion\":\n",
    "        print(\"Deletion removes rows/columns with missing values. This is simple\")\n",
    "        print(\"but can lose data, especially if missingness is high. Are you sure?\")\n",
    "        confirmation = input(\"Proceed with deletion (y/n)? \").lower()\n",
    "        if confirmation == \"y\":\n",
    "          data = data.dropna()  # Drops rows with any missing values\n",
    "          print(\"Missing values deleted!\")\n",
    "          break  # Exit the loop after successful deletion\n",
    "        else:\n",
    "          print(\"Deletion skipped based on your confirmation.\")\n",
    "\n",
    "      # User chooses imputation\n",
    "      elif method_choice == \"imputation\":\n",
    "        print(\"Imputation estimates missing values based on other data points.\")\n",
    "        print(\"There are different imputation techniques, each with advantages and disadvantages:\")\n",
    "        print(\"  - Mean/Median/Mode Imputation (simple but might not be suitable for skewed data).\")\n",
    "        print(\"  - Interpolation (estimates missing values based on surrounding values).\")\n",
    "        print(\"  - Model-based Imputation (uses machine learning to predict missing values,\")\n",
    "        print(\"     more complex but potentially more accurate).\")\n",
    "        impute_choice = input(\"Choose an imputation method (mean/median/mode/interpolation): \").lower()\n",
    "        if impute_choice in [\"mean\", \"median\", \"mode\"]:\n",
    "          # Simple imputation using mean/median/mode\n",
    "          if impute_choice == \"mean\":\n",
    "            imputation_strategy = np.mean\n",
    "          elif impute_choice == \"median\":\n",
    "            imputation_strategy = np.median\n",
    "          else:\n",
    "            imputation_strategy = np.mode\n",
    "          data = data.fillna(method=imputation_strategy)  # Fill missing values with chosen strategy\n",
    "          print(f\"Missing values imputed using {impute_choice} strategy!\")\n",
    "          break  # Exit the loop after successful imputation\n",
    "        elif impute_choice == \"interpolation\":\n",
    "          # Interpolation (using linear interpolation here)\n",
    "          data = data.interpolate(\"linear\")  # Linear interpolation for missing values\n",
    "          print(\"Missing values imputed using linear interpolation!\")\n",
    "          break  # Exit the loop after successful interpolation\n",
    "        else:\n",
    "          print(\"Invalid imputation method chosen. Skipping imputation.\")\n",
    "\n",
    "      # User chooses encoding\n",
    "      elif method_choice == \"encoding\":\n",
    "        print(\"Encoding creates a new feature indicating the presence or absence of a missing value.\")\n",
    "        print(\"This can be informative for some models but increases the number of features.\")\n",
    "        print(\"Are you sure you want to proceed with encoding?\")\n",
    "        confirmation = input(\"Proceed with encoding (y/n)? \").lower()\n",
    "        if confirmation == \"y\":\n",
    "          data = pd.get_dummies(data, dummy_na=True)  # Encode missing values as features\n",
    "          print(\"Missing values encoded as new features!\")\n",
    "          break  # Exit the loop after successful encoding\n",
    "        else:\n",
    "          print(\"Encoding skipped based on your confirmation.\")\n",
    "\n",
    "      # Invalid method chosen (loop continues)\n",
    "      else:\n",
    "        print(\"Invalid method chosen. Please try again from the available options:\")\n",
    "        print(\"- deletion\")\n",
    "        print(\"- imputation\")\n",
    "        print(\"- encoding\")\n",
    "\n",
    "  return data\n",
    "\n",
    "\n",
    "data = handle_missing_values(data.copy())  # Avoid modifying original data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this code block, we define a function called `handle_missing_values` that takes a DataFrame named `data` as input. This function helps us deal with missing values (represented by `NaN` in pandas) in our data! Let's break down what it does:**\n",
    "\n",
    "**1. Checking for Missing Values:**\n",
    "\n",
    "* First, we calculate the total number of missing values in each column of our DataFrame `data`. We do this by using the `isnull()` method, which identifies missing values, and then using `sum()` to get the total count of missing values per column. The result is stored in a variable called `null_sums`.\n",
    "\n",
    "**2. Handling No Missing Values (The Easy Case):**\n",
    "\n",
    "* If there are absolutely no missing values (`null_sums.sum() == 0`), that's great news! We'll simply print a message letting you know that there's nothing to worry about in this DataFrame.\n",
    "\n",
    "**3. Informing You and Prompting for Action (When Missing Values Exist):**\n",
    "\n",
    "* If there are missing values (`else:` block), we'll explain why it's important to address them before analyzing our data. Missing values can skew our results and lead to inaccurate conclusions.\n",
    "* Then, we'll ask you to choose a method for handling these missing values using the `input()` function. You can choose from \"deletion,\" \"imputation,\" or \"encoding.\"\n",
    "\n",
    "**4. Handling Your Choice with a Loop (The Main Logic):**\n",
    "\n",
    "* To make sure you enter a valid method, we'll use a `while True:` loop. This loop keeps iterating until you provide a valid choice.\n",
    "    * Inside the loop, we'll ask you again for your chosen method using `input()`, converting it to lowercase for case-insensitive comparison.\n",
    "    * Then, we'll check your choice using a series of `if-elif-else` statements to guide you through different options:\n",
    "\n",
    "        * **Deletion:**\n",
    "            * If you choose \"deletion,\" we'll explain that this removes rows or columns with missing values, but it can also lead to data loss, especially if there are a lot of missing values.\n",
    "            * We'll ask for confirmation. If you confirm (`confirmation == \"y\"`), we'll use `data.dropna()` to drop those rows/columns and print a success message.\n",
    "            * If you don't confirm, we'll let you know that deletion was skipped.\n",
    "        * **Imputation:**\n",
    "            * If you choose \"imputation,\" we'll explain how it estimates missing values based on other data points. We'll also provide information on different imputation techniques: mean, median, mode, and interpolation.\n",
    "            * Then, we'll ask you to choose a specific imputation method.\n",
    "            * If you select a valid imputation method (mean, median, mode, or interpolation), we'll use `data.fillna(method=imputation_strategy)` to fill in the missing values based on your chosen strategy. We'll then print a success message.\n",
    "            * If you choose an invalid imputation method, we'll let you know that imputation was skipped.\n",
    "        * **Encoding:**\n",
    "            * If you choose \"encoding,\" we'll explain how this approach creates new features in our DataFrame to indicate the presence or absence of a missing value. This can be informative for some models but can also increase the number of features we're working with.\n",
    "            * We'll ask for confirmation. If you confirm, we'll use `pd.get_dummies(data, dummy_na=True)` to encode the missing values as new features. A success message will be printed.\n",
    "            * If you don't confirm, we'll let you know that encoding was skipped.\n",
    "        * **Invalid Method:**\n",
    "            * If you enter an invalid method other than the three options, we'll display an error message and provide a list of valid options to help you choose correctly. The loop will then continue to prompt you for a valid choice.\n",
    "\n",
    "**5. Returning the Modified DataFrame:**\n",
    "\n",
    "* Finally, after successfully handling missing values based on your choice (deletion, imputation, or encoding), we'll return the modified DataFrame (`data`) that may now contain imputed values or encoded features for missing data.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "* This code provides an interactive way for us to handle missing values in our DataFrame.\n",
    "* The loop ensures you enter a valid method choice.\n",
    "* We offer basic explanations and guidance for each missing value handling option.\n",
    "* You can further enhance this code by including comments within the function and potentially adding more advanced imputation techniques.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Handling Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Detection\n",
    "\n",
    "def identify_and_handle_outliers(data):\n",
    "    \"\"\"\n",
    "    Identifies outliers and prompts user for imputation, removal, or keeping outliers.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The DataFrame containing the data.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The modified DataFrame (potentially with outliers left unchanged).\n",
    "    \"\"\"\n",
    "    numerical_cols = data.select_dtypes(include=[np.number])\n",
    "    outliers_exist = False  # Flag to track presence of outliers\n",
    "\n",
    "    for col in numerical_cols:\n",
    "        # Calculate quartiles and IQR\n",
    "        Q1 = numerical_cols[col].quantile(0.25)\n",
    "        Q3 = numerical_cols[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        # Identify outliers based on IQR outlier rule\n",
    "        lower_bound = Q1 - (1.5 * IQR)\n",
    "        upper_bound = Q3 + (1.5 * IQR)\n",
    "        outlier_count = (numerical_cols[col] < lower_bound | numerical_cols[col] > upper_bound).sum()\n",
    "\n",
    "    if outlier_count > 0:\n",
    "        outliers_exist = True\n",
    "        print(f\"Found {outlier_count} potential outliers in column '{col}'.\")\n",
    "        print(\"\"\"\n",
    "        Outlier Treatment Options:\n",
    "\n",
    "        * Imputation: Replaces outliers with estimates (mean, median, mode) to preserve data.\n",
    "        * Removal: Removes rows containing outliers, suitable for errors or irrelevant data.\n",
    "        * Keep: Leave outliers unchanged for further analysis (consider impact on results).\n",
    "\n",
    "        Choosing the right option depends on the number of outliers, their impact on analysis, and data quality.\n",
    "        \"\"\")\n",
    "        action = input(\"Do you want to (i)mpute, (r)emove, or (k)eep outliers (i/r/k)? \").lower()\n",
    "        if action == \"i\":\n",
    "            # FUTURE DEVELOPMENT: See markdown below this cell to determine which imputation method to choose.\n",
    "            # Choose imputation method\n",
    "            print(\"\"\"\n",
    "            Choosing the Right Imputation Method:\n",
    "\n",
    "            * **Mean:** Use mean if the data is normally distributed (consider histograms or normality tests). Mean is sensitive to outliers, so consider if there are extreme values that might distort the average.\n",
    "\n",
    "            * **Median:** Use median if the data is skewed (uneven distribution) or has extreme outliers. Median is less sensitive to outliers compared to mean and represents the 'middle' value in the data.\n",
    "\n",
    "            * **Mode:** Use mode for categorical data with a dominant value. Mode represents the most frequent value in the data and is suitable for non-numerical categories.\n",
    "            \"\"\")\n",
    "            imputation_method = input(\"Choose imputation method (mean/median/mode): \").lower()\n",
    "            if imputation_method == \"mean\":\n",
    "                data.loc[numerical_cols[col].index[numerical_cols[col] < lower_bound | numerical_cols[col] > upper_bound], col] = numerical_cols[col].mean()\n",
    "                print(f\"Imputing outliers in '{col}' with mean.\")\n",
    "            elif imputation_method == \"median\":\n",
    "                data.loc[numerical_cols[col].index[numerical_cols[col] < lower_bound | numerical_cols[col] > upper_bound], col] = numerical_cols[col].median()\n",
    "                print(f\"Imputing outliers in '{col}' with median.\")\n",
    "            else:\n",
    "                # Mode imputation (consider using libraries like scikit-learn for categorical data handling)\n",
    "                data.loc[numerical_cols[col].index[numerical_cols[col] < lower_bound | numerical_cols[col] > upper_bound], col] = numerical_cols[col].mode()[0]  # Assuming single most frequent value\n",
    "                print(f\"Imputing outliers in '{col}' with mode (considering first most frequent value).\")\n",
    "        elif action == \"r\":\n",
    "            # Remove rows with outliers\n",
    "            data = data[~(numerical_cols[col] < lower_bound | numerical_cols[col] > upper_bound)]\n",
    "            print(f\"Removing rows with outliers in column '{col}'.\")\n",
    "        elif action == \"k\":\n",
    "            print(f\"Keeping outliers in column '{col}' for further analysis.\")\n",
    "        else:\n",
    "            print(f\"Invalid choice. Outliers in '{col}' remain unaddressed.\")\n",
    "\n",
    "    if not outliers_exist:\n",
    "        print(\"No outliers detected in numerical columns.\")\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Handling Duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def handle_duplicates(data):\n",
    "  \"\"\"\n",
    "  Identifies duplicates, explains options, and allows user choice for handling them.\n",
    "\n",
    "  Args:\n",
    "      data (pandas.DataFrame): The DataFrame containing the data.\n",
    "\n",
    "  Returns:\n",
    "      pandas.DataFrame: The DataFrame potentially with duplicates removed.\n",
    "  \"\"\"\n",
    "\n",
    "  # Find duplicates\n",
    "  duplicates = data.duplicated()\n",
    "\n",
    "  # Check if any duplicates exist\n",
    "  if not duplicates.any():\n",
    "    print(\"No duplicate rows found in your data. Moving on...\")\n",
    "    return data\n",
    "\n",
    "  # Print a sample of duplicates (avoid overwhelming the user)\n",
    "  print(\"Found potential duplicate rows. Here are 5 samples:\")\n",
    "  print(data[duplicates].head())\n",
    "\n",
    "  # Explain duplicate handling options\n",
    "  print(\"\\nHow would you like to handle these duplicates?\")\n",
    "  print(\"  1. Remove all duplicates (keeps the first occurrence)\")\n",
    "  print(\"  2. Keep all duplicates (may skew analysis)\")\n",
    "  print(\"  3. View all duplicates (for manual selection)\")\n",
    "\n",
    "  while True:\n",
    "    choice = input(\"Enter your choice (1, 2, or 3): \")\n",
    "\n",
    "    # Handle user choice\n",
    "    if choice == \"1\":\n",
    "      print(\"Removing all duplicates (keeping the first occurrence).\")\n",
    "      data = data.drop_duplicates()\n",
    "      break  # Exit the loop after a valid choice\n",
    "    elif choice == \"2\":\n",
    "      print(\"Keeping all duplicates (may skew analysis).\")\n",
    "      break  # Exit the loop after a valid choice\n",
    "    elif choice == \"3\":\n",
    "      print(\"Here are all duplicates. Review and choose rows to keep (comma-separated indices):\")\n",
    "      print(data[duplicates])\n",
    "      keep_indices = input(\"Enter indices of rows to KEEP (or 'all' to keep all): \")\n",
    "      if keep_indices.lower() == \"all\":\n",
    "        data = data[duplicates]  # Keep all duplicates\n",
    "      else:\n",
    "        try:\n",
    "          # Convert user input to a list of integers (indices)\n",
    "          keep_indices = [int(i) for i in keep_indices.split(\",\")]\n",
    "          data = data.iloc[keep_indices]  # Keep rows based on indices\n",
    "          print(f\"Keeping rows with indices: {keep_indices}\")\n",
    "        except ValueError:\n",
    "          print(\"Invalid input. Please enter comma-separated integers or 'all'.\")\n",
    "      break  # Exit the loop after a valid choice\n",
    "    else:\n",
    "      print(\"Invalid choice. Please enter 1, 2, or 3.\")\n",
    "\n",
    "  return data\n",
    "\n",
    "# Example usage (assuming data is your DataFrame)\n",
    "data = handle_duplicates(data.copy())  # Avoid modifying original data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell defines a function called `handle_duplicates` that helps us identify and deal with duplicate rows within a Pandas DataFrame. It provides an interactive experience where we can choose how to handle these duplicates.\n",
    "\n",
    "Here's a breakdown of the functionality:\n",
    "\n",
    "1. **Function Definition:**\n",
    "   - The function takes a DataFrame (`data`) as input, containing the data you want to clean.\n",
    "   - It explains its purpose through a docstring, mentioning duplicate identification, user options, and the modified DataFrame output.\n",
    "\n",
    "2. **Finding Duplicates:**\n",
    "   - It uses the `duplicated` method on the DataFrame to identify rows that are identical copies (duplicates).\n",
    "\n",
    "3. **Checking for Duplicates:**\n",
    "   - The code checks if any duplicates exist. If not, it informs you and returns the original data without modification.\n",
    "\n",
    "4. **Printing Sample Duplicates (if any):**\n",
    "   - If duplicates are found, it avoids overwhelming you by showing only the first 5 duplicate rows for reference.\n",
    "\n",
    "5. **Explaining Duplicate Handling Options:**\n",
    "   - The function presents three choices for handling duplicates:\n",
    "     - Remove all duplicates (keeps only the first occurrence).\n",
    "     - Keep all duplicates (may skew analysis due to data redundancy).\n",
    "     - View all duplicates for manual selection (allows you to choose specific rows to keep).\n",
    "\n",
    "6. **Interactive User Choice and Handling:**\n",
    "   - A loop ensures you enter a valid choice (1, 2, or 3) before proceeding. \n",
    "   - Based on your choice:\n",
    "     - Option 1: Removes all duplicates using `drop_duplicates` and exits the loop.\n",
    "     - Option 2: Informs you about keeping all duplicates and exits the loop.\n",
    "     - Option 3 (commented out): This section would handle viewing and selecting specific duplicates to keep (explained further below).\n",
    "   - After a valid choice is made, the loop exits.\n",
    "\n",
    "7. **Option 3: View and Choose Duplicates (commented out):**\n",
    "   - This section (commented out for demonstration) would typically include code to:\n",
    "     - Print all duplicate rows in the DataFrame.\n",
    "     - Ask you to enter indices of the rows you want to **KEEP** (separated by commas) or \"all\" to keep everything.\n",
    "     - Depending on your input:\n",
    "       - If \"all\", it would keep all duplicates.\n",
    "       - If comma-separated indices, it would keep only those specific rows and update the DataFrame.\n",
    "       - If invalid input is provided, it would display an error message.\n",
    "\n",
    "By using this function, we can effectively clean your data by handling duplicate rows in a user-friendly and interactive way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dealing With Inconsistent Formatting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def handle_formatting(data):\n",
    "  \"\"\"\n",
    "  Identifies inconsistent formatting and allows user choice for handling it.\n",
    "\n",
    "  Args:\n",
    "      data (pandas.DataFrame): The DataFrame containing the data.\n",
    "\n",
    "  Returns:\n",
    "      pandas.DataFrame: The DataFrame potentially with formatting inconsistencies fixed.\n",
    "  \"\"\"\n",
    "\n",
    "  # Check for date formatting inconsistencies\n",
    "  date_cols = [col for col in data if pd.api.types.is_datetime64_dtype(data[col])]\n",
    "  if date_cols:\n",
    "    print(\"Found potential date formatting inconsistencies in columns:\")\n",
    "    for col in date_cols:\n",
    "      print(f\"  - {col}\")\n",
    "    print(\"  (Inconsistent date formats can lead to errors during analysis.)\\n\")\n",
    "\n",
    "  # Check for currency formatting inconsistencies\n",
    "  currency_cols = [col for col in data if pd.api.types.is_numeric_dtype(data[col]) and any(char in data[col] for char in r\"$£€¥₱\")]\n",
    "  if currency_cols:\n",
    "    print(\"Found potential currency formatting inconsistencies in columns:\")\n",
    "    for col in currency_cols:\n",
    "      print(f\"  - {col} (mixed currency symbols or no symbol)\")\n",
    "    print(\"  (Inconsistent currency formatting can hinder analysis.)\\n\")\n",
    "\n",
    "  # Offer choices if inconsistencies found\n",
    "  if date_cols or currency_cols:\n",
    "    choice = input(\"Would you like to attempt fixing these formatting issues (y/n)? \")\n",
    "    if choice.lower() == \"y\":\n",
    "      # Fix formatting based on user choice\n",
    "      for col in date_cols:\n",
    "        valid_choice = False\n",
    "        while not valid_choice:\n",
    "          print(f\"\\nChoose a desired date format for '{col}':\")\n",
    "          print(\"  1. YYYY-MM-DD (e.g., 2024-05-26)\")\n",
    "          print(\"  2. MM-DD-YYYY (e.g., 05-26-2024)\")\n",
    "          print(\"  3. D/M/YYYY (e.g., 26/05/2024)\")\n",
    "          print(\"  4. YYYY/MM/DD (e.g., 2024/05/26)\")\n",
    "          format_choice = input(\"Enter your choice (1, 2, etc.): \")\n",
    "          try:\n",
    "            # Convert to chosen date format (assuming choices 1, 2, 4, and 5)\n",
    "            if format_choice in (\"1\", \"2\", \"3\", \"4\"):\n",
    "              if format_choice == \"1\":\n",
    "                data[col] = pd.to_datetime(data[col], format=\"%Y-%m-%d\")\n",
    "              elif format_choice == \"2\":\n",
    "                data[col] = pd.to_datetime(data[col], format=\"%m-%d-%Y\")\n",
    "              elif format_choice == \"3\":\n",
    "                data[col] = pd.to_datetime(data[col], format=\"%d/%m/%Y\")\n",
    "              elif format_choice == \"4\":\n",
    "                data[col] = pd.to_datetime(data[col], format=\"%Y/%m/%d\")\n",
    "              valid_choice = True\n",
    "            else:\n",
    "              print(\"Invalid choice. Please choose from options 1-5.\")\n",
    "          except ValueError:\n",
    "            print(f\"Error parsing dates in '{col}'. Keeping existing format.\")\n",
    "\n",
    "      for col in currency_cols:\n",
    "        print(f\"\\nChoose a desired currency symbol for '{col}':\")\n",
    "        print(\"  1. USD ($)\")\n",
    "        print(\"  2. EUR (€)\")\n",
    "        print(\"  3. No symbol\")\n",
    "        symbol_choice = input(\"Enter your choice (1, 2, or 3): \")\n",
    "        if symbol_choice == \"1\":\n",
    "          # Replace existing symbols with '$' (assuming text data)\n",
    "          data[col] = data[col].str.replace(r\"[£€¥₱]\", \"$\", regex=True)\n",
    "        elif symbol_choice == \"2\":\n",
    "          data[col] = data[col].str.replace(r\"[£¥₱$]\", \"€\", regex=True)\n",
    "        elif symbol_choice == \"3\":\n",
    "          # Remove all currency symbols (assuming text data)\n",
    "          data[col] = data[col].str.replace(r\"[£€¥₱$]\", \"\", regex=True)\n",
    "        else:\n",
    "          print(\"Invalid choice. Keeping existing formatting for\", col)\n",
    "      print(\"Formatting potentially fixed in some columns.\")\n",
    "    else:\n",
    "      print(\"Keeping existing formatting (may cause issues during analysis).\")\n",
    "\n",
    "  return data\n",
    "\n",
    "# Example usage (assuming data is your DataFrame)\n",
    "data = handle_formatting(data.copy())  # Avoid modifying original data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell contains Python code for a function called `handle_formatting` that helps you identify and address inconsistencies in date and currency formatting within a Pandas DataFrame. It provides an interactive experience where you can choose how to fix these issues.\n",
    "\n",
    "Here's a breakdown of what the function does:\n",
    "\n",
    "1. **Checks for Formatting Inconsistencies:**\n",
    "   - It identifies columns containing datetime data types and checks for mixed date formats.\n",
    "   - It finds numeric columns containing currency symbols and checks for inconsistencies (mixed symbols or no symbol).\n",
    "\n",
    "2. **Offers Choices (if inconsistencies found):**\n",
    "   - If inconsistencies are found, we'll be prompted to choose whether to attempt fixing them (`y`) or keep the existing formatting (`n`).\n",
    "\n",
    "3. **Fixing Formatting Based on User Choice (Interactive):**\n",
    "   - If we choose to fix formatting:\n",
    "      - For columns with date inconsistencies, we can select a desired date format (e.g., YYYY-MM-DD, MM-DD-YYYY). The function attempts to convert dates in that column to the chosen format.\n",
    "      - For columns with currency inconsistencies, we can select a desired currency symbol (e.g., USD ($), EUR (€)) or choose to remove symbols entirely. The function uses string manipulation to replace existing symbols or remove them altogether.\n",
    "\n",
    "4. **Returns the DataFrame:**\n",
    "   - The function returns the potentially modified DataFrame with formatting inconsistencies (hopefully) fixed based on our choices.\n",
    "\n",
    "**Benefits of Using This Function:**\n",
    "\n",
    "- **Clean Data:** Ensures consistent date and currency formatting throughout our DataFrame, improving data quality and analysis.\n",
    "- **User-friendly:** Provides interactive choices for fixing formatting issues based on your preferences.\n",
    "- **Error Handling:** Attempts to gracefully handle potential errors during date parsing.\n",
    "\n",
    "This code snippet imports the Pandas library, calls the `handle_formatting` function on your DataFrame (`data`), and prints the potentially formatted DataFrame. Remember to make a copy of our DataFrame (`data.copy()`) to avoid modifying the original data.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
