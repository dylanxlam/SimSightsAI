{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning and data preprocessing are both crucial steps in preparing data for analysis or machine learning models. However, they serve distinct purposes within the data preparation pipeline:\n",
    "\n",
    "**Data Cleaning:**\n",
    "\n",
    "* **Focus:** Addresses inconsistencies, errors, and missing values within the data itself.\n",
    "* **Goals:**\n",
    "    * Ensure data accuracy and consistency.\n",
    "    * Improve data quality for analysis.\n",
    "    * Prepare data for further processing.\n",
    "* **Techniques:**\n",
    "    * Identifying and handling missing values (imputation, removal).\n",
    "    * Detecting and correcting outliers (winsorization, removal).\n",
    "    * Dealing with inconsistencies (e.g., formatting errors, typos).\n",
    "    * Handling invalid or irrelevant data points.\n",
    "\n",
    "**Data Preprocessing:**\n",
    "\n",
    "* **Focus:** Transforms the data into a format suitable for analysis or modeling algorithms.\n",
    "* **Goals:**\n",
    "    * Improve model performance by making data more usable.\n",
    "    * Reduce computational complexity for models.\n",
    "    * Engineer new features that might be more informative.\n",
    "* **Techniques:**\n",
    "    * Feature scaling or normalization (putting all features on the same scale).\n",
    "    * Encoding categorical variables (converting text categories to numerical values).\n",
    "    * Feature selection (choosing relevant features for modeling).\n",
    "    * Feature engineering (creating new features based on existing ones).\n",
    "\n",
    "**Key Differences:**\n",
    "\n",
    "* **Data cleaning** deals with the quality and integrity of the raw data, while **data preprocessing** focuses on transforming the data for specific modeling tasks.\n",
    "* **Data cleaning** is often more about fixing errors and inconsistencies, while **data preprocessing** involves feature engineering and preparing the data for algorithms.\n",
    "\n",
    "**In summary:**\n",
    "\n",
    "* Data cleaning is a prerequisite for data preprocessing. You clean the data before transforming it for modeling.\n",
    "* Both data cleaning and data preprocessing are essential steps for building robust and effective machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **data_handler/preprocessing.py**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Reading Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_data\n",
    "\n",
    "file = input(\"Please upload your data file. We support CSV, Excel, TSV, and JSON\")\n",
    "\n",
    "def read_data(file):\n",
    "    \"\"\"\n",
    "    Reads data from uploaded file (supports CSV, Excel, TSV, JSON).\n",
    "\n",
    "    Args:\n",
    "        file (object): The uploaded file object from Flask request.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame (or list/dict): The loaded data in a suitable format.\n",
    "    \"\"\"\n",
    "    # Identify file format based on filename extension or MIME type (consider using magic library)\n",
    "    if file.filename.endswith(\".csv\"):\n",
    "        data = pd.read_csv(file)\n",
    "    elif file.filename.endswith(\".xlsx\"):\n",
    "        data = pd.read_excel(file)\n",
    "    elif file.filename.endswith(\".tsv\"):\n",
    "        data = pd.read_csv(file, sep=\"\\t\")  # Use tab separator for TSV\n",
    "    elif file.filename.endswith(\".json\"):\n",
    "        try:\n",
    "            data = json.load(file)  # Assuming JSON data represents a list or dictionary\n",
    "        except json.JSONDecodeError:\n",
    "            raise ValueError(\"Invalid JSON format. Please check your data.\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please upload CSV, Excel, TSV, or JSON files.\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Can add logic for handling image formats (consider for future development)\n",
    "    # elif file.content_type.startswith(\"image/\"):  # Check for image content type\n",
    "    #     # Implement image loading and pre-processing logic here (libraries like OpenCV)\n",
    "    #     # Return a suitable data structure for image data\n",
    "    #     pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Descriptive Statistics\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data_descriptive_statistics \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mdescribe()\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDescriptive Statistics of Your Data:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, data_descriptive_statistics)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Descriptive Statistics\n",
    "data_descriptive_statistics = data.describe()\n",
    "print(\"Descriptive Statistics of Your Data:\\n\", data_descriptive_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will display summary statistics for numerical columns in your data, including:\n",
    "\n",
    "- count: The number of non-null values in each column.\n",
    "- mean: The average value.\n",
    "- std: The standard deviation.\n",
    "- min: The minimum value.\n",
    "- 25%: The first quartile (25th percentile).\n",
    "- 50%: The median (50th percentile).\n",
    "- 75%: The third quartile (75th percentile).\n",
    "- max: The maximum value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how the summary statistics from `data.describe()` can inform data cleaning methods for robust modeling:\n",
    "\n",
    "**1. Analyzing Central Tendency:**\n",
    "\n",
    "* **Mean & Median:** These statistics represent the \"average\" value in a column. Significant deviations between mean and median can indicate a skewed distribution.\n",
    "\n",
    "* **Skew:** This statistic directly measures the skewness of the data. A positive skew indicates more data points concentrated towards lower values, while negative skew suggests a tail towards higher values.\n",
    "\n",
    "**Implications for Cleaning:**\n",
    "\n",
    "* Skewed data can affect the performance of some machine learning models. Depending on the model and the severity of the skew, you might consider data transformation (e.g., log transformation) or using models robust to skewed data.\n",
    "\n",
    "**2. Understanding Dispersion:**\n",
    "\n",
    "* **Standard Deviation (Std):** This statistic shows how spread out the data is from the mean. A high Std indicates high variability, while a low Std suggests the data is clustered around the mean.\n",
    "\n",
    "* **Minimum & Maximum:** These values reveal the range of the data. Outliers (values far from the rest) can be identified by comparing them to the IQR (Interquartile Range) or a certain number of standard deviations from the mean.\n",
    "\n",
    "**Implications for Cleaning:**\n",
    "\n",
    "* Outliers can significantly impact some models. You might need to decide on handling outliers through winsorization (capping them to a certain threshold), removal, or using models less sensitive to outliers.\n",
    "\n",
    "**3. Exploring Data Types:**\n",
    "\n",
    "* **Data Types:** `data.describe()` often shows the data type (e.g., int, float) of each column. Inconsistencies or incorrect data types (e.g., dates stored as text) can lead to errors in modeling.\n",
    "\n",
    "**Implications for Cleaning:**\n",
    "\n",
    "* You might need to convert data types (e.g., text to numeric for numerical features, handling dates appropriately) to ensure compatibility with modeling algorithms.\n",
    "\n",
    "**Overall, `data.describe()` provides a high-level overview of the data's central tendency, dispersion, and potential issues like missing values and outliers. By analyzing these statistics, you can identify areas where data cleaning is necessary to prepare your data for robust modeling.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Null Values for each column\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m missing_values \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39misnull()\u001b[39m.\u001b[39msum()\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMissing/Null Values for Each Column/Feature of Your Data:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, missing_values)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Null Values for each column\n",
    "missing_values = data.isnull().sum()\n",
    "print(\"Missing/Null Values for Each Column/Feature of Your Data:\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Types\n",
    "data_types = data.dtypes\n",
    "print(\"Data Types for Each Column in Your Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focus: Data cleaning addresses inconsistencies, errors, and missing values within the data itself.\n",
    "\n",
    "Data Type Conversion: In this context, converting data types is often a cleaning step when the data type is incorrect or incompatible with how the data should be represented.\n",
    "Examples:\n",
    "- Inconsistent date formats (text to datetime).\n",
    "- Text values in numerical columns (text to numerical).\n",
    "- Incorrect data types due to import issues (e.g., strings instead of integers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Detection\n",
    "\n",
    "def identify_and_handle_outliers(data):\n",
    "    \"\"\"\n",
    "    Identifies outliers and prompts user for imputation, removal, or keeping outliers.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The DataFrame containing the data.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The modified DataFrame (potentially with outliers left unchanged).\n",
    "    \"\"\"\n",
    "    numerical_cols = data.select_dtypes(include=[np.number])\n",
    "    outliers_exist = False  # Flag to track presence of outliers\n",
    "\n",
    "    for col in numerical_cols:\n",
    "        # Calculate quartiles and IQR\n",
    "        Q1 = numerical_cols[col].quantile(0.25)\n",
    "        Q3 = numerical_cols[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        # Identify outliers based on IQR outlier rule\n",
    "        lower_bound = Q1 - (1.5 * IQR)\n",
    "        upper_bound = Q3 + (1.5 * IQR)\n",
    "        outlier_count = (numerical_cols[col] < lower_bound | numerical_cols[col] > upper_bound).sum()\n",
    "\n",
    "    if outlier_count > 0:\n",
    "        outliers_exist = True\n",
    "        print(f\"Found {outlier_count} potential outliers in column '{col}'.\")\n",
    "        print(\"\"\"\n",
    "        Outlier Treatment Options:\n",
    "\n",
    "        * Imputation: Replaces outliers with estimates (mean, median, mode) to preserve data.\n",
    "        * Removal: Removes rows containing outliers, suitable for errors or irrelevant data.\n",
    "        * Keep: Leave outliers unchanged for further analysis (consider impact on results).\n",
    "\n",
    "        Choosing the right option depends on the number of outliers, their impact on analysis, and data quality.\n",
    "        \"\"\")\n",
    "        action = input(\"Do you want to (i)mpute, (r)emove, or (k)eep outliers (i/r/k)? \").lower()\n",
    "        if action == \"i\":\n",
    "            # FUTURE DEVELOPMENT: See markdown below this cell to determine which imputation method to choose.\n",
    "            # Choose imputation method\n",
    "            print(\"\"\"\n",
    "            Choosing the Right Imputation Method:\n",
    "\n",
    "            * **Mean:** Use mean if the data is normally distributed (consider histograms or normality tests). Mean is sensitive to outliers, so consider if there are extreme values that might distort the average.\n",
    "\n",
    "            * **Median:** Use median if the data is skewed (uneven distribution) or has extreme outliers. Median is less sensitive to outliers compared to mean and represents the 'middle' value in the data.\n",
    "\n",
    "            * **Mode:** Use mode for categorical data with a dominant value. Mode represents the most frequent value in the data and is suitable for non-numerical categories.\n",
    "            \"\"\")\n",
    "            imputation_method = input(\"Choose imputation method (mean/median/mode): \").lower()\n",
    "            if imputation_method == \"mean\":\n",
    "                data.loc[numerical_cols[col].index[numerical_cols[col] < lower_bound | numerical_cols[col] > upper_bound], col] = numerical_cols[col].mean()\n",
    "                print(f\"Imputing outliers in '{col}' with mean.\")\n",
    "            elif imputation_method == \"median\":\n",
    "                data.loc[numerical_cols[col].index[numerical_cols[col] < lower_bound | numerical_cols[col] > upper_bound], col] = numerical_cols[col].median()\n",
    "                print(f\"Imputing outliers in '{col}' with median.\")\n",
    "            else:\n",
    "                # Mode imputation (consider using libraries like scikit-learn for categorical data handling)\n",
    "                data.loc[numerical_cols[col].index[numerical_cols[col] < lower_bound | numerical_cols[col] > upper_bound], col] = numerical_cols[col].mode()[0]  # Assuming single most frequent value\n",
    "                print(f\"Imputing outliers in '{col}' with mode (considering first most frequent value).\")\n",
    "        elif action == \"r\":\n",
    "            # Remove rows with outliers\n",
    "            data = data[~(numerical_cols[col] < lower_bound | numerical_cols[col] > upper_bound)]\n",
    "            print(f\"Removing rows with outliers in column '{col}'.\")\n",
    "        elif action == \"k\":\n",
    "            print(f\"Keeping outliers in column '{col}' for further analysis.\")\n",
    "        else:\n",
    "            print(f\"Invalid choice. Outliers in '{col}' remain unaddressed.\")\n",
    "\n",
    "    if not outliers_exist:\n",
    "        print(\"No outliers detected in numerical columns.\")\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most efficient way to assess normality and skewness in your data columns depends on a few factors:\n",
    "\n",
    "**1. Number of Columns:**\n",
    "\n",
    "* **Few Columns:** If you have a small number of columns (less than 10), visual inspection using histograms and QQ plots might be the most efficient approach. These techniques are easy to understand and interpret, providing a quick grasp of the data distribution.\n",
    "\n",
    "* **Many Columns:** With a large number of columns (more than 10), visual inspection becomes cumbersome. Here, statistical tests like the Shapiro-Wilk test can be more efficient. You can calculate the test statistic and p-value for each column to identify potential deviations from normality. A threshold for the p-value (e.g., 0.05) can be used to decide if the data is likely non-normal.\n",
    "\n",
    "**2. Desired Level of Detail:**\n",
    "\n",
    "* **Basic Assessment:** If you just need a quick indication of normality or skewness, histograms and statistical tests with p-values provide a sufficient level of detail.\n",
    "\n",
    "* **Detailed Analysis:** For a more in-depth analysis, you can combine both approaches. Start with histograms and QQ plots to get a visual sense of the distribution, and then follow up with statistical tests to confirm your observations or explore borderline cases with p-values close to the chosen threshold.\n",
    "\n",
    "Here's a breakdown of the efficiency considerations:\n",
    "\n",
    "| Method | Efficiency for Few Columns | Efficiency for Many Columns | Level of Detail |\n",
    "|---|---|---|---|\n",
    "| Histograms & QQ Plots | High (easy to interpret visually) | Low (time-consuming for many columns) | High (visual assessment of shape) |\n",
    "| Statistical Tests | Low (requires calculations) | High (efficient for many columns) | Moderate (p-value indicates normality likelihood) |\n",
    "\n",
    "**Combined Approach:**\n",
    "\n",
    "In practice, a combination of visual inspection and statistical tests often offers the best balance between efficiency and detail. Start with histograms and QQ plots for a quick overview, then use statistical tests for more rigorous confirmation, especially when dealing with many columns.\n",
    "\n",
    "Here are some additional factors to consider:\n",
    "\n",
    "* **Computational Resources:** If computational resources are limited, visual methods might be preferred. Statistical tests, especially for large datasets, can require more processing power.\n",
    "* **Domain Knowledge:** If you have domain knowledge about the data, you might have an initial expectation about the normality of certain features. This can guide your choice of method, focusing on tests for features where normality is critical for your analysis.\n",
    "\n",
    "Ultimately, the most efficient approach depends on your specific needs and the size of your dataset. Combining visual and statistical methods often provides a comprehensive and efficient way to assess normality and skewness in your data columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Here are some prominent machine learning methods used to analyze photos (mainly for assessing data normality):\n",
    "\n",
    "**1. Convolutional Neural Networks (CNNs):**\n",
    "\n",
    "* This is a dominant approach for image analysis tasks like:\n",
    "    * **Image Classification:** Classifying images based on their content (e.g., cat, dog, car). CNNs excel at recognizing patterns and features in images.\n",
    "    * **Object Detection:** Identifying and locating specific objects within an image (e.g., identifying pedestrians, traffic signs). CNNs can not only classify objects but also pinpoint their location in the image.\n",
    "    * **Image Segmentation:** Dividing an image into regions corresponding to different objects or parts of the scene. This helps in understanding the image composition and relationships between objects.\n",
    "\n",
    "**2. Generative Adversarial Networks (GANs):**\n",
    "\n",
    "* These involve two competing neural networks:\n",
    "    * **Generator:** Creates new images that resemble real photos based on the data it's trained on.\n",
    "    * **Discriminator:** Tries to distinguish between real photos and the generated images.\n",
    "* Applications include:\n",
    "    * **Image Inpainting:** Filling in missing parts of an image realistically.\n",
    "    * **Style Transfer:** Applying the style of one image to another (e.g., making a photo look like a painting).\n",
    "    * **Super-Resolution:** Creating a higher-resolution version of an image.\n",
    "\n",
    "**3. Autoencoders:**\n",
    "\n",
    "* These are neural networks that learn to compress an input image into a lower-dimensional representation (encoded) and then reconstruct the original image from that encoding (decoded).\n",
    "* Applications include:\n",
    "    * **Anomaly Detection:** Identifying unusual or unexpected images by comparing the reconstruction error.\n",
    "    * **Dimensionality Reduction:** Representing images in a more compact form for storage or processing.\n",
    "    * **Data Denoising:** Removing noise from images.\n",
    "\n",
    "**4. Object Recognition with Transformers:**\n",
    "\n",
    "* While CNNs are traditional leaders, transformers, known for their success in natural language processing, are making inroads in image recognition. \n",
    "* These models process image data by breaking it down into smaller patches and analyzing relationships between them, potentially offering an alternative approach to CNNs for certain tasks.\n",
    "\n",
    "**Choosing the Right Method:**\n",
    "\n",
    "The choice of machine learning method for photo analysis depends on the specific task you want to accomplish. Here are some factors to consider:\n",
    "\n",
    "* **Task Type:** Classification, object detection, segmentation, image generation, etc.\n",
    "* **Data Availability:** The amount and type of labeled photo data available for training.\n",
    "* **Computational Resources:** The complexity of the model and the hardware required to train and run it.\n",
    "\n",
    "**Additional Considerations:**\n",
    "\n",
    "* **Explainability:** Some methods, like CNNs, can be challenging to interpret in terms of how they arrive at their decisions. This is an ongoing area of research in machine learning. \n",
    "* **Transfer Learning:** Pre-trained models on large image datasets can be fine-tuned for specific tasks, reducing training time and potentially improving performance.\n",
    "\n",
    "By understanding these methods and the factors influencing their choice, you can leverage machine learning to extract valuable insights from your photo data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Class Imbalance in preprocessing section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_data(data):\n",
    "    # Your data cleaning logic here (handling missing values, outliers, etc.)\n",
    "    pass\n",
    "\n",
    "def pre_process_data(data, task):\n",
    "    # Specific pre-processing steps based on chosen task\n",
    "    pass\n",
    "\n",
    "def handle_data_upload():\n",
    "    # Load data from uploaded file\n",
    "    data = ...\n",
    "    \n",
    "    # Preprocess data\n",
    "    # cleaned_data = preprocessing.clean_data(data)\n",
    "    # preprocessed_data = preprocessing.pre_process_data(cleaned_data, chosen_task)\n",
    "    \n",
    "    # Use preprocessed data for further analysis or modeling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
